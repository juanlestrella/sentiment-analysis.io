{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nour Mansour and Juan Estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Data Collection\n",
    "\n",
    "Step 1: Get the tsv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data is saved in the same folder as the project. Then read data from tsv file\n",
    "data = pd.read_csv(\"train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Data Processing\n",
    "\n",
    "Step 1: Columns required: Label, Comments, subreddit, parent comment\n",
    "\n",
    "Step 2: Create a Dataframe containing an even amount of sarcastic and non sarcastic \n",
    "comments, amount of data is 505413 each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the indices after rows with NA values are dropped\n",
    "data.dropna(inplace=True)\n",
    "data.drop(['author', 'score', 'ups', 'downs', 'date', 'created_utc', 'subreddit', 'parent_comment'], axis = 1, inplace = True)\n",
    "data.reset_index(inplace = True)\n",
    "data.drop(['index'], axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really wanted to use the entire dataset, unfortunately our laptops weren't strong enough to perform the different types of data manipulation and analysis we wanted. \n",
    "As a solution, we decided to sample 10% of the dataset each time we ran the program. 10% was actually the largest sample we could get without our Jupyter kernel dying from overflowing Docker's allocated memory resources (we set the memory for Docker to the max value possible given our laptops' specs!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop(data[data.comment =='You forgot the'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='you forgot the'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='you dropped this:'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='You dropped this:'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='Forgot the'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='You forgot'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='You forgot your'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='You dropped this'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='I think you forgot the'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='You forgot this:'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='You dropped your'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='I forgot the'].index, inplace=True)\n",
    "# data.drop(data[data.comment =='you forgot'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>411777</th>\n",
       "      <td>0</td>\n",
       "      <td>Ah ok, I thought you were referring to his vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185659</th>\n",
       "      <td>0</td>\n",
       "      <td>i'll give you a safari mesh ak.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469723</th>\n",
       "      <td>0</td>\n",
       "      <td>But 22lr is is 22 caliber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318700</th>\n",
       "      <td>0</td>\n",
       "      <td>If you're looking for a large capacity shotgun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231416</th>\n",
       "      <td>0</td>\n",
       "      <td>This is seriously impressive!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                            comment\n",
       "0 411777      0  Ah ok, I thought you were referring to his vid...\n",
       "  185659      0                    i'll give you a safari mesh ak.\n",
       "  469723      0                          But 22lr is is 22 caliber\n",
       "  318700      0  If you're looking for a large capacity shotgun...\n",
       "  231416      0                      This is seriously impressive!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine size and replacement values\n",
    "size = int(len(data)*1/100)# sample size\n",
    "replace = True  # with replacement\n",
    "# Function used to pick random rows from the dataset\n",
    "# We used the groupby('label') function to choose equal numbers \n",
    "# of sarcastic and non-sarcastic comments\n",
    "fun = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "chosen_data = data.groupby('label', as_index=False).apply(fun)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ah ok, I thought you were referring to his vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i'll give you a safari mesh ak.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>But 22lr is is 22 caliber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>If you're looking for a large capacity shotgun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This is seriously impressive!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0  Ah ok, I thought you were referring to his vid...\n",
       "1      0                    i'll give you a safari mesh ak.\n",
       "2      0                          But 22lr is is 22 caliber\n",
       "3      0  If you're looking for a large capacity shotgun...\n",
       "4      0                      This is seriously impressive!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the indexes to remove the grouping into 0s and 1s\n",
    "chosen_data.reset_index(inplace = True)\n",
    "# Delete the columns that include the names of the indexes created from the groups\n",
    "chosen_data.drop(['level_0', 'level_1'], inplace = True, axis = 1)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20209</th>\n",
       "      <td>1</td>\n",
       "      <td>gibt auch nichts besseres als sich am bahnhof ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210</th>\n",
       "      <td>1</td>\n",
       "      <td>No man, weed makes you lazy and out of shape, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Added new items to the lighthouse loot table\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20212</th>\n",
       "      <td>1</td>\n",
       "      <td>9/11 so I can see Bush planting the bombs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20213</th>\n",
       "      <td>1</td>\n",
       "      <td>You mean *Power* Peppermint... gotta have thos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                            comment\n",
       "20209      1  gibt auch nichts besseres als sich am bahnhof ...\n",
       "20210      1  No man, weed makes you lazy and out of shape, ...\n",
       "20211      1     \"Added new items to the lighthouse loot table\"\n",
       "20212      1          9/11 so I can see Bush planting the bombs\n",
       "20213      1  You mean *Power* Peppermint... gotta have thos..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Exploratory Analysis & Data Visualization\n",
    "\n",
    "We made sure we got equal number of sarcastic and non-sarcastic comments. From the following histogram, we notice we got 50,000 comments from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore distribution of the data by label (0 -> non-sarcastic, 1 -> sarcastic)\n",
    "plt.hist(chosen_data.label)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Comments by Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10107</td>\n",
       "      <td>9879</td>\n",
       "      <td>No.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10107</td>\n",
       "      <td>9897</td>\n",
       "      <td>You forgot the</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment                            \n",
       "        count unique             top freq\n",
       "label                                    \n",
       "0       10107   9879             No.    8\n",
       "1       10107   9897  You forgot the   27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore distribution of comments by label \n",
    "chosen_data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/16/4d247e27c55a7b6412e7c4c86f2500ae61afcbf5932b9e3491f8462f8d9e/nltk-3.4.4.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/41/c8/31/48ace4468e236e0e8435f30d33e43df48594e4d53e367cf061\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.4\n"
     ]
    }
   ],
   "source": [
    "#Import NLTK library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #Use this to get rid of meaningless words like \"the, and, a\"\n",
    "from nltk.tokenize import word_tokenize #Split by word\n",
    "from nltk.tokenize import sent_tokenize #Split by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all the comment column is str data type\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "\n",
    "#non_sarcastic = required_data.loc[required_data['label'] == 0]\n",
    "#sarcastic = required_data.loc[required_data['label'] == 1]\n",
    "\n",
    "#drop rows with na values on the comment column\n",
    "#non_sarcastic['comment'].dropna(inplace=True)\n",
    "#sarcastic['comment'].dropna(inplace=True)\n",
    "\n",
    "#Make sure all the comment column is str data type\n",
    "#non_sarcastic['comment'] = non_sarcastic['comment'].astype(str)\n",
    "#sarcastic['comment'] = sarcastic['comment'].astype(str)\n",
    "#print(len(non_sarcastic), len(sarcastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ah, ok, ,, I, thought, you, were, referring, ...</td>\n",
       "      <td>[(Ah, NNP), (ok, PRP), (,, ,), (I, PRP), (thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[i, 'll, give, you, a, safari, mesh, ak, .]</td>\n",
       "      <td>[(i, NN), ('ll, MD), (give, VB), (you, PRP), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[But, 22lr, is, is, 22, caliber]</td>\n",
       "      <td>[(But, CC), (22lr, CD), (is, VBZ), (is, VBZ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[If, you, 're, looking, for, a, large, capacit...</td>\n",
       "      <td>[(If, IN), (you, PRP), ('re, VBP), (looking, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[This, is, seriously, impressive, !]</td>\n",
       "      <td>[(This, DT), (is, VBZ), (seriously, RB), (impr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Ah, ok, ,, I, thought, you, were, referring, ...   \n",
       "1      0        [i, 'll, give, you, a, safari, mesh, ak, .]   \n",
       "2      0                   [But, 22lr, is, is, 22, caliber]   \n",
       "3      0  [If, you, 're, looking, for, a, large, capacit...   \n",
       "4      0               [This, is, seriously, impressive, !]   \n",
       "\n",
       "                                             POS_tag  \n",
       "0  [(Ah, NNP), (ok, PRP), (,, ,), (I, PRP), (thou...  \n",
       "1  [(i, NN), ('ll, MD), (give, VB), (you, PRP), (...  \n",
       "2  [(But, CC), (22lr, CD), (is, VBZ), (is, VBZ), ...  \n",
       "3  [(If, IN), (you, PRP), ('re, VBP), (looking, V...  \n",
       "4  [(This, DT), (is, VBZ), (seriously, RB), (impr...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the comments into words\n",
    "chosen_data['comment'] = chosen_data['comment'].apply(word_tokenize)\n",
    "# Apply Parts of Speech tagging on the words\n",
    "chosen_data['POS_tag'] = chosen_data['comment'].apply(nltk.pos_tag)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ah, ok, ,, I, thought, you, were, referring, ...</td>\n",
       "      <td>[(Ah, NNP), (ok, PRP), (,, ,), (I, PRP), (thou...</td>\n",
       "      <td>[ah, ok, ,, i, thought, you, were, refer, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[i, 'll, give, you, a, safari, mesh, ak, .]</td>\n",
       "      <td>[(i, NN), ('ll, MD), (give, VB), (you, PRP), (...</td>\n",
       "      <td>[i, ll, give, you, a, safari, mesh, ak, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[But, 22lr, is, is, 22, caliber]</td>\n",
       "      <td>[(But, CC), (22lr, CD), (is, VBZ), (is, VBZ), ...</td>\n",
       "      <td>[but, 22lr, is, is, 22, calib]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[If, you, 're, looking, for, a, large, capacit...</td>\n",
       "      <td>[(If, IN), (you, PRP), ('re, VBP), (looking, V...</td>\n",
       "      <td>[if, you, re, look, for, a, larg, capac, shotg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[This, is, seriously, impressive, !]</td>\n",
       "      <td>[(This, DT), (is, VBZ), (seriously, RB), (impr...</td>\n",
       "      <td>[this, is, serious, impress, !]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Ah, ok, ,, I, thought, you, were, referring, ...   \n",
       "1      0        [i, 'll, give, you, a, safari, mesh, ak, .]   \n",
       "2      0                   [But, 22lr, is, is, 22, caliber]   \n",
       "3      0  [If, you, 're, looking, for, a, large, capacit...   \n",
       "4      0               [This, is, seriously, impressive, !]   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Ah, NNP), (ok, PRP), (,, ,), (I, PRP), (thou...   \n",
       "1  [(i, NN), ('ll, MD), (give, VB), (you, PRP), (...   \n",
       "2  [(But, CC), (22lr, CD), (is, VBZ), (is, VBZ), ...   \n",
       "3  [(If, IN), (you, PRP), ('re, VBP), (looking, V...   \n",
       "4  [(This, DT), (is, VBZ), (seriously, RB), (impr...   \n",
       "\n",
       "                                                stem  \n",
       "0  [ah, ok, ,, i, thought, you, were, refer, to, ...  \n",
       "1         [i, ll, give, you, a, safari, mesh, ak, .]  \n",
       "2                     [but, 22lr, is, is, 22, calib]  \n",
       "3  [if, you, re, look, for, a, larg, capac, shotg...  \n",
       "4                    [this, is, serious, impress, !]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply stemming on the tokenized comments to get the roots of the words\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "chosen_data['stem'] = chosen_data['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After confirming our suspicions that using TF-IDF on the raw tokenized comments would yield very modest results, we decided to continue exploring how we can fix our models. First, we used chunking to break up the comments into more meaningful noun phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reg ex to chunk the sentences into noun phrases\n",
    "# np_chunking = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "# chunk_parser = nltk.RegexpParser(np_chunking)\n",
    "# chosen_data['noun_phrase_chunk'] = chosen_data['POS_tag'].apply(chunk_parser.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the chunked sentences column groups the nouns into groups with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# chosen_data.iloc[0:10, 2:5:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chosen_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tag = []\n",
    "for index, row in chosen_data.iterrows():\n",
    "    joined_tag.append(' '.join([word + \"_\" + pos for word, pos in row['POS_tag']]))\n",
    "chosen_data['joined_POS_tag'] = joined_tag.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>joined_POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ah, ok, ,, I, thought, you, were, referring, ...</td>\n",
       "      <td>[(Ah, NNP), (ok, PRP), (,, ,), (I, PRP), (thou...</td>\n",
       "      <td>[ah, ok, ,, i, thought, you, were, refer, to, ...</td>\n",
       "      <td>Ah_NNP ok_PRP ,_, I_PRP thought_VBD you_PRP we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[i, 'll, give, you, a, safari, mesh, ak, .]</td>\n",
       "      <td>[(i, NN), ('ll, MD), (give, VB), (you, PRP), (...</td>\n",
       "      <td>[i, ll, give, you, a, safari, mesh, ak, .]</td>\n",
       "      <td>i_NN 'll_MD give_VB you_PRP a_DT safari_JJ mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[But, 22lr, is, is, 22, caliber]</td>\n",
       "      <td>[(But, CC), (22lr, CD), (is, VBZ), (is, VBZ), ...</td>\n",
       "      <td>[but, 22lr, is, is, 22, calib]</td>\n",
       "      <td>But_CC 22lr_CD is_VBZ is_VBZ 22_CD caliber_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[If, you, 're, looking, for, a, large, capacit...</td>\n",
       "      <td>[(If, IN), (you, PRP), ('re, VBP), (looking, V...</td>\n",
       "      <td>[if, you, re, look, for, a, larg, capac, shotg...</td>\n",
       "      <td>If_IN you_PRP 're_VBP looking_VBG for_IN a_DT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[This, is, seriously, impressive, !]</td>\n",
       "      <td>[(This, DT), (is, VBZ), (seriously, RB), (impr...</td>\n",
       "      <td>[this, is, serious, impress, !]</td>\n",
       "      <td>This_DT is_VBZ seriously_RB impressive_JJ !_.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Ah, ok, ,, I, thought, you, were, referring, ...   \n",
       "1      0        [i, 'll, give, you, a, safari, mesh, ak, .]   \n",
       "2      0                   [But, 22lr, is, is, 22, caliber]   \n",
       "3      0  [If, you, 're, looking, for, a, large, capacit...   \n",
       "4      0               [This, is, seriously, impressive, !]   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Ah, NNP), (ok, PRP), (,, ,), (I, PRP), (thou...   \n",
       "1  [(i, NN), ('ll, MD), (give, VB), (you, PRP), (...   \n",
       "2  [(But, CC), (22lr, CD), (is, VBZ), (is, VBZ), ...   \n",
       "3  [(If, IN), (you, PRP), ('re, VBP), (looking, V...   \n",
       "4  [(This, DT), (is, VBZ), (seriously, RB), (impr...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [ah, ok, ,, i, thought, you, were, refer, to, ...   \n",
       "1         [i, ll, give, you, a, safari, mesh, ak, .]   \n",
       "2                     [but, 22lr, is, is, 22, calib]   \n",
       "3  [if, you, re, look, for, a, larg, capac, shotg...   \n",
       "4                    [this, is, serious, impress, !]   \n",
       "\n",
       "                                      joined_POS_tag  \n",
       "0  Ah_NNP ok_PRP ,_, I_PRP thought_VBD you_PRP we...  \n",
       "1  i_NN 'll_MD give_VB you_PRP a_DT safari_JJ mes...  \n",
       "2      But_CC 22lr_CD is_VBZ is_VBZ 22_CD caliber_NN  \n",
       "3  If_IN you_PRP 're_VBP looking_VBG for_IN a_DT ...  \n",
       "4      This_DT is_VBZ seriously_RB impressive_JJ !_.  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we'll need to lemmatize the words yet\n",
    "\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#data['lemma'] = data['tokenized_by_word'].apply(WordNetLemmatizer)\n",
    "# df14.append(df24.append(df34.append(df44)))\n",
    "# df14.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Sarcasm\n",
    "\n",
    "We were interested in trying different models for predicting a sarcastic comment.\n",
    "To determine the features and labels for the analysis, we decided to look at the comment itself as a feature and use the given sarcastic vs non-sarcastic classification as our label.\n",
    "\n",
    "Various articles and studies looked at sarcasm and attempted to predict using different features. For instance, an online tutorial only looked at TF-IDF as a feature, so we decided we wanted to experiment with that and see if we would get anything remotely close to their findings. Worth noting, however, that they looked at tweets with the #sarcasm tag, their dataset was significantly smaller than ours (N= ), and their training vs test data split was slightly unusual (training = 95%, test = 5%). \n",
    "\n",
    "To create a more reliable model, we decided to split our data into the more common 80-20 split for training and test subsets respectively. We also chose three main models to compare, acknowledging that some of them might fare slightly more poorly compared to the others. We also assumed TF-IDF vectorization of the comments alone would not be a good predictor for whether a comment is sarcastic or not. \n",
    "\n",
    "The three models we picked were:\n",
    "1. Logistic regression\n",
    "2. Support Vector Machine\n",
    "3. Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>joined_POS_tag</th>\n",
       "      <th>joined_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ah, ok, ,, I, thought, you, were, referring, ...</td>\n",
       "      <td>[(Ah, NNP), (ok, PRP), (,, ,), (I, PRP), (thou...</td>\n",
       "      <td>[ah, ok, ,, i, thought, you, were, refer, to, ...</td>\n",
       "      <td>Ah_NNP ok_PRP ,_, I_PRP thought_VBD you_PRP we...</td>\n",
       "      <td>ah ok , i thought you were refer to his video .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[i, 'll, give, you, a, safari, mesh, ak, .]</td>\n",
       "      <td>[(i, NN), ('ll, MD), (give, VB), (you, PRP), (...</td>\n",
       "      <td>[i, ll, give, you, a, safari, mesh, ak, .]</td>\n",
       "      <td>i_NN 'll_MD give_VB you_PRP a_DT safari_JJ mes...</td>\n",
       "      <td>i ll give you a safari mesh ak .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[But, 22lr, is, is, 22, caliber]</td>\n",
       "      <td>[(But, CC), (22lr, CD), (is, VBZ), (is, VBZ), ...</td>\n",
       "      <td>[but, 22lr, is, is, 22, calib]</td>\n",
       "      <td>But_CC 22lr_CD is_VBZ is_VBZ 22_CD caliber_NN</td>\n",
       "      <td>but 22lr is is 22 calib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[If, you, 're, looking, for, a, large, capacit...</td>\n",
       "      <td>[(If, IN), (you, PRP), ('re, VBP), (looking, V...</td>\n",
       "      <td>[if, you, re, look, for, a, larg, capac, shotg...</td>\n",
       "      <td>If_IN you_PRP 're_VBP looking_VBG for_IN a_DT ...</td>\n",
       "      <td>if you re look for a larg capac shotgun a saig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[This, is, seriously, impressive, !]</td>\n",
       "      <td>[(This, DT), (is, VBZ), (seriously, RB), (impr...</td>\n",
       "      <td>[this, is, serious, impress, !]</td>\n",
       "      <td>This_DT is_VBZ seriously_RB impressive_JJ !_.</td>\n",
       "      <td>this is serious impress !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Ah, ok, ,, I, thought, you, were, referring, ...   \n",
       "1      0        [i, 'll, give, you, a, safari, mesh, ak, .]   \n",
       "2      0                   [But, 22lr, is, is, 22, caliber]   \n",
       "3      0  [If, you, 're, looking, for, a, large, capacit...   \n",
       "4      0               [This, is, seriously, impressive, !]   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Ah, NNP), (ok, PRP), (,, ,), (I, PRP), (thou...   \n",
       "1  [(i, NN), ('ll, MD), (give, VB), (you, PRP), (...   \n",
       "2  [(But, CC), (22lr, CD), (is, VBZ), (is, VBZ), ...   \n",
       "3  [(If, IN), (you, PRP), ('re, VBP), (looking, V...   \n",
       "4  [(This, DT), (is, VBZ), (seriously, RB), (impr...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [ah, ok, ,, i, thought, you, were, refer, to, ...   \n",
       "1         [i, ll, give, you, a, safari, mesh, ak, .]   \n",
       "2                     [but, 22lr, is, is, 22, calib]   \n",
       "3  [if, you, re, look, for, a, larg, capac, shotg...   \n",
       "4                    [this, is, serious, impress, !]   \n",
       "\n",
       "                                      joined_POS_tag  \\\n",
       "0  Ah_NNP ok_PRP ,_, I_PRP thought_VBD you_PRP we...   \n",
       "1  i_NN 'll_MD give_VB you_PRP a_DT safari_JJ mes...   \n",
       "2      But_CC 22lr_CD is_VBZ is_VBZ 22_CD caliber_NN   \n",
       "3  If_IN you_PRP 're_VBP looking_VBG for_IN a_DT ...   \n",
       "4      This_DT is_VBZ seriously_RB impressive_JJ !_.   \n",
       "\n",
       "                                         joined_stem  \n",
       "0    ah ok , i thought you were refer to his video .  \n",
       "1                   i ll give you a safari mesh ak .  \n",
       "2                            but 22lr is is 22 calib  \n",
       "3  if you re look for a larg capac shotgun a saig...  \n",
       "4                          this is serious impress !  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features\n",
    "chosen_data['joined_stem'] = chosen_data['stem'].apply(lambda x: ' '.join(x))\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(chosen_data[['joined_POS_tag', 'joined_stem']] , chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joined_POS_tag</th>\n",
       "      <th>joined_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9581</th>\n",
       "      <td>``_`` Of_IN course_NN lot_NN 's_POS of_IN peop...</td>\n",
       "      <td>`` of cours lot 's of peopl disagre with me ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13722</th>\n",
       "      <td>This_DT is_VBZ basically_RB 4Chan_CD</td>\n",
       "      <td>this is basic 4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15254</th>\n",
       "      <td>Some_DT shit_NN happened_VBD and_CC the_DT poo...</td>\n",
       "      <td>some shit happen and the poor guy got reset 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6956</th>\n",
       "      <td>It_PRP 's_VBZ still_RB got_VBD a_DT great_JJ w...</td>\n",
       "      <td>it 's still got a great wine region ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>It_PRP 's_VBZ obvious_JJ ,_, NiP_NNP got_VBD t...</td>\n",
       "      <td>it 's obvious , nip got them to join the serve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          joined_POS_tag  \\\n",
       "9581   ``_`` Of_IN course_NN lot_NN 's_POS of_IN peop...   \n",
       "13722               This_DT is_VBZ basically_RB 4Chan_CD   \n",
       "15254  Some_DT shit_NN happened_VBD and_CC the_DT poo...   \n",
       "6956   It_PRP 's_VBZ still_RB got_VBD a_DT great_JJ w...   \n",
       "19980  It_PRP 's_VBZ obvious_JJ ,_, NiP_NNP got_VBD t...   \n",
       "\n",
       "                                             joined_stem  \n",
       "9581      `` of cours lot 's of peopl disagre with me ''  \n",
       "13722                                this is basic 4chan  \n",
       "15254  some shit happen and the poor guy got reset 2 ...  \n",
       "6956             it 's still got a great wine region ...  \n",
       "19980  it 's obvious , nip got them to join the serve...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF to vectorize the data\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stem_vectorizer = TfidfVectorizer() # ask about max features\n",
    "tfidf_stem = stem_vectorizer.fit_transform(X_train.joined_stem)\n",
    "\n",
    "pos_vectorizer = TfidfVectorizer()\n",
    "tfidf_pos = pos_vectorizer.fit_transform(X_train.joined_POS_tag)\n",
    "\n",
    "combined_2 = sp.hstack([tfidf_stem, tfidf_pos], format = 'csr')\n",
    "\n",
    "\n",
    "# tfidf_train_x = vectorizer.fit_transform(X_train)\n",
    "# print(vectorizer.fit_transform(chosen_data['joined_stem']).toarray().shape)\n",
    "\n",
    "# The dimensions from the tfidf vectorization doesn't fit the dataframe\n",
    "# I'm not sure how to include both features in the model\n",
    "\n",
    "# chosen_data['tfidf_stem'] = vectorizer.fit_transform(chosen_data['joined_stem']).toarray()\n",
    "# chosen_data['tfidf_pos'] = vectorizer.fit_transform(chosen_data['joined_POS_tag']).toarray()\n",
    "# chosen_data.head()\n",
    "# features = list(features)\n",
    "# X = vectorizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stem_vectorizer = TfidfVectorizer(max_features = 39079) # ask about max features\n",
    "test_tfidf_stem = stem_vectorizer.fit_transform(X_test.joined_stem)\n",
    "\n",
    "test_pos_vectorizer = TfidfVectorizer(max_features = 39079)\n",
    "test_tfidf_pos = pos_vectorizer.fit_transform(X_test.joined_POS_tag)\n",
    "\n",
    "test_combined_2 = sp.hstack([test_tfidf_stem, test_tfidf_pos], format = 'csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627172098200482\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 16388 features per sample; expecting 39079",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2b088e499b18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Logistic regression score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_combined_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 262\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 16388 features per sample; expecting 39079"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(combined_2, y_train))\n",
    "print(log_clf.score(test_combined_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the features and labels for the models\n",
    "# features = chosen_data['stem'].apply(lambda x: ' '.join(x))\n",
    "# labels = chosen_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 300).fit(combined_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vectorized the comments here and used the TF-IDF list as the feature in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = chosen_data[['tfidf_stem', 'tfidf_pos']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x1.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.get_feature_names())\n",
    "# print(X.shape)\n",
    "x2 = vectorizer.fit_transform(chosen_data['joined_POS_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x1)\n",
    "# print(x2)\n",
    "# print(x1.shape)\n",
    "# print(x2.shape)\n",
    "# print(x1.toarray())\n",
    "# print(list(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9fe0b356d478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# d = {'stem': x1.toarray(), 'pos': list(x2)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "# d = {'stem': x1.toarray(), 'pos': list(x2)}\n",
    "\n",
    "X = [x1, x2].as_matrix()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "estimators = [('tfidf', TfidfVectorizer())]\n",
    "combined = FeatureUnion(estimators)\n",
    "combined\n",
    "# features = chosen_data.iloc[:, 3:6:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joined_POS_tag</th>\n",
       "      <th>joined_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh_UH ,_, they_PRP do_VBP ._.</td>\n",
       "      <td>oh , they do .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>makes_VBZ it_PRP way_NN to_TO much_JJ red_JJ a...</td>\n",
       "      <td>make it way to much red and yellow imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Um_NN ..._: What_WP ?_.</td>\n",
       "      <td>um ... what ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For_IN select_JJ teams_NNS :_: 49ers_CD somewh...</td>\n",
       "      <td>for select team : 49er somewher in the top 4 r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If_IN you_PRP make_VBP coffee_NN often_RB you_...</td>\n",
       "      <td>if you make coffe often you alreadi have all t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      joined_POS_tag  \\\n",
       "0                      Oh_UH ,_, they_PRP do_VBP ._.   \n",
       "1  makes_VBZ it_PRP way_NN to_TO much_JJ red_JJ a...   \n",
       "2                            Um_NN ..._: What_WP ?_.   \n",
       "3  For_IN select_JJ teams_NNS :_: 49ers_CD somewh...   \n",
       "4  If_IN you_PRP make_VBP coffee_NN often_RB you_...   \n",
       "\n",
       "                                         joined_stem  \n",
       "0                                     oh , they do .  \n",
       "1             make it way to much red and yellow imo  \n",
       "2                                      um ... what ?  \n",
       "3  for select team : 49er somewher in the top 4 r...  \n",
       "4  if you make coffe often you alreadi have all t...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = chosen_data.iloc[:, 5:7]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.fit_transform(features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x2 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "X = combined.fit_transform(features)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)\n",
    "# X.toarray() # it only works if it's an array but why does it become all 0s?\n",
    "X = chosen_data.iloc[:, 3:6:2]\n",
    "X\n",
    "# chosen_data.iloc[:, 3:6:2]\n",
    "# print(chosen_data.iloc[0, 3])\n",
    "X['stem'] = X['stem'].apply(lambda x: x.toarray())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>noun_phrase_chunk</th>\n",
       "      <th>joined_POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[How, 's, his, wife, ?]</td>\n",
       "      <td>[(How, WRB), ('s, VBZ), (his, PRP$), (wife, NN...</td>\n",
       "      <td>[how, 's, his, wife, ?]</td>\n",
       "      <td>[(How, WRB), ('s, VBZ), (his, PRP$), [(wife, N...</td>\n",
       "      <td>How_WRB 's_VBZ his_PRP$ wife_NN ?_.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[And, let, Artemis, traps, act, like, springs,...</td>\n",
       "      <td>[(And, CC), (let, VB), (Artemis, NNP), (traps,...</td>\n",
       "      <td>[and, let, artemi, trap, act, like, spring, wh...</td>\n",
       "      <td>[(And, CC), (let, VB), (Artemis, NNP), (traps,...</td>\n",
       "      <td>And_CC let_VB Artemis_NNP traps_VB act_NN like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[my, first, thought, :, ``, Why, is, a, wookie...</td>\n",
       "      <td>[(my, PRP$), (first, JJ), (thought, NN), (:, :...</td>\n",
       "      <td>[my, first, thought, :, ``, whi, is, a, wooki,...</td>\n",
       "      <td>[(my, PRP$), [(first, JJ), (thought, NN)], (:,...</td>\n",
       "      <td>my_PRP$ first_JJ thought_NN :_: ``_`` Why_WRB ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[In, my, experience, the, people, who, make, t...</td>\n",
       "      <td>[(In, IN), (my, PRP$), (experience, NN), (the,...</td>\n",
       "      <td>[in, my, experi, the, peopl, who, make, those,...</td>\n",
       "      <td>[(In, IN), (my, PRP$), [(experience, NN)], (th...</td>\n",
       "      <td>In_IN my_PRP$ experience_NN the_DT people_NNS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Could, n't, find, it, I, guess, ?]</td>\n",
       "      <td>[(Could, MD), (n't, RB), (find, VB), (it, PRP)...</td>\n",
       "      <td>[could, n't, find, it, i, guess, ?]</td>\n",
       "      <td>[(Could, MD), (n't, RB), (find, VB), (it, PRP)...</td>\n",
       "      <td>Could_MD n't_RB find_VB it_PRP I_PRP guess_RB ?_.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0                            [How, 's, his, wife, ?]   \n",
       "1      0  [And, let, Artemis, traps, act, like, springs,...   \n",
       "2      0  [my, first, thought, :, ``, Why, is, a, wookie...   \n",
       "3      0  [In, my, experience, the, people, who, make, t...   \n",
       "4      0                [Could, n't, find, it, I, guess, ?]   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(How, WRB), ('s, VBZ), (his, PRP$), (wife, NN...   \n",
       "1  [(And, CC), (let, VB), (Artemis, NNP), (traps,...   \n",
       "2  [(my, PRP$), (first, JJ), (thought, NN), (:, :...   \n",
       "3  [(In, IN), (my, PRP$), (experience, NN), (the,...   \n",
       "4  [(Could, MD), (n't, RB), (find, VB), (it, PRP)...   \n",
       "\n",
       "                                                stem  \\\n",
       "0                            [how, 's, his, wife, ?]   \n",
       "1  [and, let, artemi, trap, act, like, spring, wh...   \n",
       "2  [my, first, thought, :, ``, whi, is, a, wooki,...   \n",
       "3  [in, my, experi, the, peopl, who, make, those,...   \n",
       "4                [could, n't, find, it, i, guess, ?]   \n",
       "\n",
       "                                   noun_phrase_chunk  \\\n",
       "0  [(How, WRB), ('s, VBZ), (his, PRP$), [(wife, N...   \n",
       "1  [(And, CC), (let, VB), (Artemis, NNP), (traps,...   \n",
       "2  [(my, PRP$), [(first, JJ), (thought, NN)], (:,...   \n",
       "3  [(In, IN), (my, PRP$), [(experience, NN)], (th...   \n",
       "4  [(Could, MD), (n't, RB), (find, VB), (it, PRP)...   \n",
       "\n",
       "                                      joined_POS_tag  \n",
       "0                How_WRB 's_VBZ his_PRP$ wife_NN ?_.  \n",
       "1  And_CC let_VB Artemis_NNP traps_VB act_NN like...  \n",
       "2  my_PRP$ first_JJ thought_NN :_: ``_`` Why_WRB ...  \n",
       "3  In_IN my_PRP$ experience_NN the_DT people_NNS ...  \n",
       "4  Could_MD n't_RB find_VB it_PRP I_PRP guess_RB ?_.  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test values\n",
    "\n",
    "log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5003711952487008\n",
      "0.49851632047477745\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(X_train, y_train))\n",
    "print(log_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "\n",
    "svm_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5003711952487008\n",
      "0.49851632047477745\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.score(X_train, y_train))\n",
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:  0.5003711952487008\n",
      "test score:  0.5024727992087042\n"
     ]
    }
   ],
   "source": [
    "print(\"training score: \", rf_clf.score(X_train, y_train))\n",
    "print(\"test score: \", rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = chosen_data['joined_POS_tag']\n",
    "#features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = list(features2)\n",
    "X2 = vectorizer.fit_transform(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<151614x95627 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1564163 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-638289c5a63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X2' is not defined"
     ]
    }
   ],
   "source": [
    "print(X2, \"\\n\")\n",
    "print(X2.toarray(), \"\\n\")\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7819541433412207\n",
      "0.6916861788081654\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(X_train, y_train))\n",
    "print(log_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8720515124782547\n",
      "0.6900702437093955\n"
     ]
    }
   ],
   "source": [
    "# SVM model\n",
    "svm_clf = LinearSVC()\n",
    "\n",
    "# Training the model\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "print(svm_clf.score(X_train, y_train))\n",
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980196387201029\n",
      "0.6772746759885235\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 10)\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "print(rf_clf.score(X_train, y_train))\n",
    "print(rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nour Mansour and Juan Estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Data Collection\n",
    "\n",
    "Step 1: Get the tsv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data is saved in the same folder as the project. Then read data from tsv file\n",
    "data = pd.read_csv(\"train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Data Processing\n",
    "\n",
    "Step 1: Columns required: Label, Comments, subreddit, parent comment\n",
    "\n",
    "Step 2: Create a Dataframe containing an even amount of sarcastic and non sarcastic \n",
    "comments, amount of data is 505413 each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the indices after rows with NA values are dropped\n",
    "data.dropna(inplace=True)\n",
    "data.drop(['author', 'score', 'ups', 'downs', 'date', 'created_utc', 'subreddit', 'parent_comment'], axis = 1, inplace = True)\n",
    "data.reset_index(inplace = True)\n",
    "data.drop(['index'], axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really wanted to use the entire dataset, unfortunately our laptops weren't strong enough to perform the different types of data manipulation and analysis we wanted. \n",
    "As a solution, we decided to sample 10% of the dataset each time we ran the program. 10% was actually the largest sample we could get without our Jupyter kernel dying from overflowing Docker's allocated memory resources (we set the memory for Docker to the max value possible given our laptops' specs!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data.comment =='You forgot the'].index, inplace=True)\n",
    "data.drop(data[data.comment =='you forgot the'].index, inplace=True)\n",
    "data.drop(data[data.comment =='you dropped this:'].index, inplace=True)\n",
    "data.drop(data[data.comment =='You dropped this:'].index, inplace=True)\n",
    "data.drop(data[data.comment =='Forgot the'].index, inplace=True)\n",
    "data.drop(data[data.comment =='You forgot'].index, inplace=True)\n",
    "data.drop(data[data.comment =='You forgot your'].index, inplace=True)\n",
    "data.drop(data[data.comment =='You dropped this'].index, inplace=True)\n",
    "data.drop(data[data.comment =='I think you forgot the'].index, inplace=True)\n",
    "data.drop(data[data.comment =='You forgot this:'].index, inplace=True)\n",
    "data.drop(data[data.comment =='You dropped your'].index, inplace=True)\n",
    "data.drop(data[data.comment =='I forgot the'].index, inplace=True)\n",
    "data.drop(data[data.comment =='you forgot'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>429141</th>\n",
       "      <td>0</td>\n",
       "      <td>sooo you're on a destiny reddit talking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405462</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Hey, if little Joey's dead, then I got no rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545778</th>\n",
       "      <td>0</td>\n",
       "      <td>Scandinavian?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773429</th>\n",
       "      <td>0</td>\n",
       "      <td>That doesn't sound very supportive on his part.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004209</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the hell YouTube results contain these vid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                            comment\n",
       "0 429141       0  sooo you're on a destiny reddit talking about ...\n",
       "  405462       0  \"Hey, if little Joey's dead, then I got no rea...\n",
       "  545778       0                                      Scandinavian?\n",
       "  773429       0    That doesn't sound very supportive on his part.\n",
       "  1004209      0  Why the hell YouTube results contain these vid..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine size and replacement values\n",
    "size = int(len(data)*2.5/100)# sample size\n",
    "replace = True  # with replacement\n",
    "# Function used to pick random rows from the dataset\n",
    "# We used the groupby('label') function to choose equal numbers \n",
    "# of sarcastic and non-sarcastic comments\n",
    "fun = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "chosen_data = data.groupby('label', as_index=False).apply(fun)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sooo you're on a destiny reddit talking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Hey, if little Joey's dead, then I got no rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Scandinavian?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>That doesn't sound very supportive on his part.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the hell YouTube results contain these vid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0  sooo you're on a destiny reddit talking about ...\n",
       "1      0  \"Hey, if little Joey's dead, then I got no rea...\n",
       "2      0                                      Scandinavian?\n",
       "3      0    That doesn't sound very supportive on his part.\n",
       "4      0  Why the hell YouTube results contain these vid..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the indexes to remove the grouping into 0s and 1s\n",
    "chosen_data.reset_index(inplace = True)\n",
    "# Delete the columns that include the names of the indexes created from the groups\n",
    "chosen_data.drop(['level_0', 'level_1'], inplace = True, axis = 1)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50325</th>\n",
       "      <td>1</td>\n",
       "      <td>But she's his *~mommy~* wiiiiife!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50326</th>\n",
       "      <td>1</td>\n",
       "      <td>Both are almost never reported because we woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50327</th>\n",
       "      <td>1</td>\n",
       "      <td>But there is a link between video games and mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50328</th>\n",
       "      <td>1</td>\n",
       "      <td>Because creationism is defensible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50329</th>\n",
       "      <td>1</td>\n",
       "      <td>here, i think you dropped this:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                            comment\n",
       "50325      1                  But she's his *~mommy~* wiiiiife!\n",
       "50326      1  Both are almost never reported because we woul...\n",
       "50327      1  But there is a link between video games and mi...\n",
       "50328      1                 Because creationism is defensible.\n",
       "50329      1                    here, i think you dropped this:"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Exploratory Analysis & Data Visualization\n",
    "\n",
    "We made sure we got equal number of sarcastic and non-sarcastic comments. From the following histogram, we notice we got 50,000 comments from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore distribution of the data by label (0 -> non-sarcastic, 1 -> sarcastic)\n",
    "plt.hist(chosen_data.label)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Comments by Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25165</td>\n",
       "      <td>24068</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25165</td>\n",
       "      <td>24374</td>\n",
       "      <td>He forgot the</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment                           \n",
       "        count unique            top freq\n",
       "label                                   \n",
       "0       25165  24068           Yes.   21\n",
       "1       25165  24374  He forgot the    7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore distribution of comments by label \n",
    "chosen_data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "#Import NLTK library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #Use this to get rid of meaningless words like \"the, and, a\"\n",
    "from nltk.tokenize import word_tokenize #Split by word\n",
    "from nltk.tokenize import sent_tokenize #Split by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all the comment column is str data type\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "\n",
    "#non_sarcastic = required_data.loc[required_data['label'] == 0]\n",
    "#sarcastic = required_data.loc[required_data['label'] == 1]\n",
    "\n",
    "#drop rows with na values on the comment column\n",
    "#non_sarcastic['comment'].dropna(inplace=True)\n",
    "#sarcastic['comment'].dropna(inplace=True)\n",
    "\n",
    "#Make sure all the comment column is str data type\n",
    "#non_sarcastic['comment'] = non_sarcastic['comment'].astype(str)\n",
    "#sarcastic['comment'] = sarcastic['comment'].astype(str)\n",
    "#print(len(non_sarcastic), len(sarcastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>tok_comment</th>\n",
       "      <th>POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sooo you're on a destiny reddit talking about ...</td>\n",
       "      <td>[sooo, you, 're, on, a, destiny, reddit, talki...</td>\n",
       "      <td>[(sooo, NN), (you, PRP), ('re, VBP), (on, IN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Hey, if little Joey's dead, then I got no rea...</td>\n",
       "      <td>[``, Hey, ,, if, little, Joey, 's, dead, ,, th...</td>\n",
       "      <td>[(``, ``), (Hey, NNP), (,, ,), (if, IN), (litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Scandinavian?</td>\n",
       "      <td>[Scandinavian, ?]</td>\n",
       "      <td>[(Scandinavian, JJ), (?, .)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>That doesn't sound very supportive on his part.</td>\n",
       "      <td>[That, does, n't, sound, very, supportive, on,...</td>\n",
       "      <td>[(That, DT), (does, VBZ), (n't, RB), (sound, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the hell YouTube results contain these vid...</td>\n",
       "      <td>[Why, the, hell, YouTube, results, contain, th...</td>\n",
       "      <td>[(Why, WRB), (the, DT), (hell, NN), (YouTube, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  sooo you're on a destiny reddit talking about ...   \n",
       "1      0  \"Hey, if little Joey's dead, then I got no rea...   \n",
       "2      0                                      Scandinavian?   \n",
       "3      0    That doesn't sound very supportive on his part.   \n",
       "4      0  Why the hell YouTube results contain these vid...   \n",
       "\n",
       "                                         tok_comment  \\\n",
       "0  [sooo, you, 're, on, a, destiny, reddit, talki...   \n",
       "1  [``, Hey, ,, if, little, Joey, 's, dead, ,, th...   \n",
       "2                                  [Scandinavian, ?]   \n",
       "3  [That, does, n't, sound, very, supportive, on,...   \n",
       "4  [Why, the, hell, YouTube, results, contain, th...   \n",
       "\n",
       "                                             POS_tag  \n",
       "0  [(sooo, NN), (you, PRP), ('re, VBP), (on, IN),...  \n",
       "1  [(``, ``), (Hey, NNP), (,, ,), (if, IN), (litt...  \n",
       "2                       [(Scandinavian, JJ), (?, .)]  \n",
       "3  [(That, DT), (does, VBZ), (n't, RB), (sound, V...  \n",
       "4  [(Why, WRB), (the, DT), (hell, NN), (YouTube, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the comments into words\n",
    "chosen_data['tok_comment'] = chosen_data['comment'].apply(word_tokenize)\n",
    "# Apply Parts of Speech tagging on the words\n",
    "chosen_data['POS_tag'] = chosen_data['tok_comment'].apply(nltk.pos_tag)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>tok_comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sooo you're on a destiny reddit talking about ...</td>\n",
       "      <td>[sooo, you, 're, on, a, destiny, reddit, talki...</td>\n",
       "      <td>[(sooo, NN), (you, PRP), ('re, VBP), (on, IN),...</td>\n",
       "      <td>[sooo, you, re, on, a, destini, reddit, talk, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Hey, if little Joey's dead, then I got no rea...</td>\n",
       "      <td>[``, Hey, ,, if, little, Joey, 's, dead, ,, th...</td>\n",
       "      <td>[(``, ``), (Hey, NNP), (,, ,), (if, IN), (litt...</td>\n",
       "      <td>[``, hey, ,, if, littl, joey, 's, dead, ,, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Scandinavian?</td>\n",
       "      <td>[Scandinavian, ?]</td>\n",
       "      <td>[(Scandinavian, JJ), (?, .)]</td>\n",
       "      <td>[scandinavian, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>That doesn't sound very supportive on his part.</td>\n",
       "      <td>[That, does, n't, sound, very, supportive, on,...</td>\n",
       "      <td>[(That, DT), (does, VBZ), (n't, RB), (sound, V...</td>\n",
       "      <td>[that, doe, n't, sound, veri, support, on, his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the hell YouTube results contain these vid...</td>\n",
       "      <td>[Why, the, hell, YouTube, results, contain, th...</td>\n",
       "      <td>[(Why, WRB), (the, DT), (hell, NN), (YouTube, ...</td>\n",
       "      <td>[whi, the, hell, youtub, result, contain, thes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  sooo you're on a destiny reddit talking about ...   \n",
       "1      0  \"Hey, if little Joey's dead, then I got no rea...   \n",
       "2      0                                      Scandinavian?   \n",
       "3      0    That doesn't sound very supportive on his part.   \n",
       "4      0  Why the hell YouTube results contain these vid...   \n",
       "\n",
       "                                         tok_comment  \\\n",
       "0  [sooo, you, 're, on, a, destiny, reddit, talki...   \n",
       "1  [``, Hey, ,, if, little, Joey, 's, dead, ,, th...   \n",
       "2                                  [Scandinavian, ?]   \n",
       "3  [That, does, n't, sound, very, supportive, on,...   \n",
       "4  [Why, the, hell, YouTube, results, contain, th...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(sooo, NN), (you, PRP), ('re, VBP), (on, IN),...   \n",
       "1  [(``, ``), (Hey, NNP), (,, ,), (if, IN), (litt...   \n",
       "2                       [(Scandinavian, JJ), (?, .)]   \n",
       "3  [(That, DT), (does, VBZ), (n't, RB), (sound, V...   \n",
       "4  [(Why, WRB), (the, DT), (hell, NN), (YouTube, ...   \n",
       "\n",
       "                                                stem  \n",
       "0  [sooo, you, re, on, a, destini, reddit, talk, ...  \n",
       "1  [``, hey, ,, if, littl, joey, 's, dead, ,, the...  \n",
       "2                                  [scandinavian, ?]  \n",
       "3  [that, doe, n't, sound, veri, support, on, his...  \n",
       "4  [whi, the, hell, youtub, result, contain, thes...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply stemming on the tokenized comments to get the roots of the words\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "chosen_data['stem'] = chosen_data['tok_comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After confirming our suspicions that using TF-IDF on the raw tokenized comments would yield very modest results, we decided to continue exploring how we can fix our models. First, we used chunking to break up the comments into more meaningful noun phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reg ex to chunk the sentences into noun phrases\n",
    "# np_chunking = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "# chunk_parser = nltk.RegexpParser(np_chunking)\n",
    "# chosen_data['noun_phrase_chunk'] = chosen_data['POS_tag'].apply(chunk_parser.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the chunked sentences column groups the nouns into groups with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# chosen_data.iloc[0:10, 2:5:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chosen_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tag = []\n",
    "for index, row in chosen_data.iterrows():\n",
    "    joined_tag.append(' '.join([word + \"_\" + pos for word, pos in row['POS_tag']]))\n",
    "chosen_data['joined_POS_tag'] = joined_tag.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>tok_comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>joined_POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sooo you're on a destiny reddit talking about ...</td>\n",
       "      <td>[sooo, you, 're, on, a, destiny, reddit, talki...</td>\n",
       "      <td>[(sooo, NN), (you, PRP), ('re, VBP), (on, IN),...</td>\n",
       "      <td>[sooo, you, re, on, a, destini, reddit, talk, ...</td>\n",
       "      <td>sooo_NN you_PRP 're_VBP on_IN a_DT destiny_JJ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Hey, if little Joey's dead, then I got no rea...</td>\n",
       "      <td>[``, Hey, ,, if, little, Joey, 's, dead, ,, th...</td>\n",
       "      <td>[(``, ``), (Hey, NNP), (,, ,), (if, IN), (litt...</td>\n",
       "      <td>[``, hey, ,, if, littl, joey, 's, dead, ,, the...</td>\n",
       "      <td>``_`` Hey_NNP ,_, if_IN little_JJ Joey_NNP 's_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Scandinavian?</td>\n",
       "      <td>[Scandinavian, ?]</td>\n",
       "      <td>[(Scandinavian, JJ), (?, .)]</td>\n",
       "      <td>[scandinavian, ?]</td>\n",
       "      <td>Scandinavian_JJ ?_.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>That doesn't sound very supportive on his part.</td>\n",
       "      <td>[That, does, n't, sound, very, supportive, on,...</td>\n",
       "      <td>[(That, DT), (does, VBZ), (n't, RB), (sound, V...</td>\n",
       "      <td>[that, doe, n't, sound, veri, support, on, his...</td>\n",
       "      <td>That_DT does_VBZ n't_RB sound_VB very_RB suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the hell YouTube results contain these vid...</td>\n",
       "      <td>[Why, the, hell, YouTube, results, contain, th...</td>\n",
       "      <td>[(Why, WRB), (the, DT), (hell, NN), (YouTube, ...</td>\n",
       "      <td>[whi, the, hell, youtub, result, contain, thes...</td>\n",
       "      <td>Why_WRB the_DT hell_NN YouTube_NNP results_NNS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  sooo you're on a destiny reddit talking about ...   \n",
       "1      0  \"Hey, if little Joey's dead, then I got no rea...   \n",
       "2      0                                      Scandinavian?   \n",
       "3      0    That doesn't sound very supportive on his part.   \n",
       "4      0  Why the hell YouTube results contain these vid...   \n",
       "\n",
       "                                         tok_comment  \\\n",
       "0  [sooo, you, 're, on, a, destiny, reddit, talki...   \n",
       "1  [``, Hey, ,, if, little, Joey, 's, dead, ,, th...   \n",
       "2                                  [Scandinavian, ?]   \n",
       "3  [That, does, n't, sound, very, supportive, on,...   \n",
       "4  [Why, the, hell, YouTube, results, contain, th...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(sooo, NN), (you, PRP), ('re, VBP), (on, IN),...   \n",
       "1  [(``, ``), (Hey, NNP), (,, ,), (if, IN), (litt...   \n",
       "2                       [(Scandinavian, JJ), (?, .)]   \n",
       "3  [(That, DT), (does, VBZ), (n't, RB), (sound, V...   \n",
       "4  [(Why, WRB), (the, DT), (hell, NN), (YouTube, ...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [sooo, you, re, on, a, destini, reddit, talk, ...   \n",
       "1  [``, hey, ,, if, littl, joey, 's, dead, ,, the...   \n",
       "2                                  [scandinavian, ?]   \n",
       "3  [that, doe, n't, sound, veri, support, on, his...   \n",
       "4  [whi, the, hell, youtub, result, contain, thes...   \n",
       "\n",
       "                                      joined_POS_tag  \n",
       "0  sooo_NN you_PRP 're_VBP on_IN a_DT destiny_JJ ...  \n",
       "1  ``_`` Hey_NNP ,_, if_IN little_JJ Joey_NNP 's_...  \n",
       "2                                Scandinavian_JJ ?_.  \n",
       "3  That_DT does_VBZ n't_RB sound_VB very_RB suppo...  \n",
       "4  Why_WRB the_DT hell_NN YouTube_NNP results_NNS...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we'll need to lemmatize the words yet\n",
    "\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#data['lemma'] = data['tokenized_by_word'].apply(WordNetLemmatizer)\n",
    "# df14.append(df24.append(df34.append(df44)))\n",
    "# df14.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: textblob in /opt/conda/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1 in /opt/conda/lib/python3.7/site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "(50330,)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob_list = []\n",
    "for index, row in chosen_data.iterrows():\n",
    "    blob_list.append(TextBlob(row['comment']))\n",
    "chosen_data['textblob'] = blob_list.copy()\n",
    "\n",
    "chosen_data.head()\n",
    "\n",
    "polarity_list = []\n",
    "for index, row in chosen_data.iterrows():\n",
    "    polarity_list.append(row['textblob'].sentiment.polarity)\n",
    "\n",
    "chosen_data['polarity'] = polarity_list.copy()\n",
    "\n",
    "chosen_data.head()\n",
    "\n",
    "print(chosen_data['polarity'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Sarcasm\n",
    "\n",
    "We were interested in trying different models for predicting a sarcastic comment.\n",
    "To determine the features and labels for the analysis, we decided to look at the comment itself as a feature and use the given sarcastic vs non-sarcastic classification as our label.\n",
    "\n",
    "Various articles and studies looked at sarcasm and attempted to predict using different features. For instance, an online tutorial only looked at TF-IDF as a feature, so we decided we wanted to experiment with that and see if we would get anything remotely close to their findings. Worth noting, however, that they looked at tweets with the #sarcasm tag, their dataset was significantly smaller than ours (N= ), and their training vs test data split was slightly unusual (training = 95%, test = 5%). \n",
    "\n",
    "To create a more reliable model, we decided to split our data into the more common 80-20 split for training and test subsets respectively. We also chose three main models to compare, acknowledging that some of them might fare slightly more poorly compared to the others. We also assumed TF-IDF vectorization of the comments alone would not be a good predictor for whether a comment is sarcastic or not. \n",
    "\n",
    "The three models we picked were:\n",
    "1. Logistic regression\n",
    "2. Support Vector Machine\n",
    "3. Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>tok_comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>joined_POS_tag</th>\n",
       "      <th>textblob</th>\n",
       "      <th>polarity</th>\n",
       "      <th>joined_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sooo you're on a destiny reddit talking about ...</td>\n",
       "      <td>[sooo, you, 're, on, a, destiny, reddit, talki...</td>\n",
       "      <td>[(sooo, NN), (you, PRP), ('re, VBP), (on, IN),...</td>\n",
       "      <td>[sooo, you, re, on, a, destini, reddit, talk, ...</td>\n",
       "      <td>sooo_NN you_PRP 're_VBP on_IN a_DT destiny_JJ ...</td>\n",
       "      <td>(s, o, o, o,  , y, o, u, ', r, e,  , o, n,  , ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sooo you re on a destini reddit talk about fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Hey, if little Joey's dead, then I got no rea...</td>\n",
       "      <td>[``, Hey, ,, if, little, Joey, 's, dead, ,, th...</td>\n",
       "      <td>[(``, ``), (Hey, NNP), (,, ,), (if, IN), (litt...</td>\n",
       "      <td>[``, hey, ,, if, littl, joey, 's, dead, ,, the...</td>\n",
       "      <td>``_`` Hey_NNP ,_, if_IN little_JJ Joey_NNP 's_...</td>\n",
       "      <td>(\", H, e, y, ,,  , i, f,  , l, i, t, t, l, e, ...</td>\n",
       "      <td>-0.083712</td>\n",
       "      <td>`` hey , if littl joey 's dead , then i got no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Scandinavian?</td>\n",
       "      <td>[Scandinavian, ?]</td>\n",
       "      <td>[(Scandinavian, JJ), (?, .)]</td>\n",
       "      <td>[scandinavian, ?]</td>\n",
       "      <td>Scandinavian_JJ ?_.</td>\n",
       "      <td>(S, c, a, n, d, i, n, a, v, i, a, n, ?)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>scandinavian ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>That doesn't sound very supportive on his part.</td>\n",
       "      <td>[That, does, n't, sound, very, supportive, on,...</td>\n",
       "      <td>[(That, DT), (does, VBZ), (n't, RB), (sound, V...</td>\n",
       "      <td>[that, doe, n't, sound, veri, support, on, his...</td>\n",
       "      <td>That_DT does_VBZ n't_RB sound_VB very_RB suppo...</td>\n",
       "      <td>(T, h, a, t,  , d, o, e, s, n, ', t,  , s, o, ...</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>that doe n't sound veri support on his part .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the hell YouTube results contain these vid...</td>\n",
       "      <td>[Why, the, hell, YouTube, results, contain, th...</td>\n",
       "      <td>[(Why, WRB), (the, DT), (hell, NN), (YouTube, ...</td>\n",
       "      <td>[whi, the, hell, youtub, result, contain, thes...</td>\n",
       "      <td>Why_WRB the_DT hell_NN YouTube_NNP results_NNS...</td>\n",
       "      <td>(W, h, y,  , t, h, e,  , h, e, l, l,  , Y, o, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>whi the hell youtub result contain these video...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  sooo you're on a destiny reddit talking about ...   \n",
       "1      0  \"Hey, if little Joey's dead, then I got no rea...   \n",
       "2      0                                      Scandinavian?   \n",
       "3      0    That doesn't sound very supportive on his part.   \n",
       "4      0  Why the hell YouTube results contain these vid...   \n",
       "\n",
       "                                         tok_comment  \\\n",
       "0  [sooo, you, 're, on, a, destiny, reddit, talki...   \n",
       "1  [``, Hey, ,, if, little, Joey, 's, dead, ,, th...   \n",
       "2                                  [Scandinavian, ?]   \n",
       "3  [That, does, n't, sound, very, supportive, on,...   \n",
       "4  [Why, the, hell, YouTube, results, contain, th...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(sooo, NN), (you, PRP), ('re, VBP), (on, IN),...   \n",
       "1  [(``, ``), (Hey, NNP), (,, ,), (if, IN), (litt...   \n",
       "2                       [(Scandinavian, JJ), (?, .)]   \n",
       "3  [(That, DT), (does, VBZ), (n't, RB), (sound, V...   \n",
       "4  [(Why, WRB), (the, DT), (hell, NN), (YouTube, ...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [sooo, you, re, on, a, destini, reddit, talk, ...   \n",
       "1  [``, hey, ,, if, littl, joey, 's, dead, ,, the...   \n",
       "2                                  [scandinavian, ?]   \n",
       "3  [that, doe, n't, sound, veri, support, on, his...   \n",
       "4  [whi, the, hell, youtub, result, contain, thes...   \n",
       "\n",
       "                                      joined_POS_tag  \\\n",
       "0  sooo_NN you_PRP 're_VBP on_IN a_DT destiny_JJ ...   \n",
       "1  ``_`` Hey_NNP ,_, if_IN little_JJ Joey_NNP 's_...   \n",
       "2                                Scandinavian_JJ ?_.   \n",
       "3  That_DT does_VBZ n't_RB sound_VB very_RB suppo...   \n",
       "4  Why_WRB the_DT hell_NN YouTube_NNP results_NNS...   \n",
       "\n",
       "                                            textblob  polarity  \\\n",
       "0  (s, o, o, o,  , y, o, u, ', r, e,  , o, n,  , ...  0.000000   \n",
       "1  (\", H, e, y, ,,  , i, f,  , l, i, t, t, l, e, ... -0.083712   \n",
       "2            (S, c, a, n, d, i, n, a, v, i, a, n, ?)  0.000000   \n",
       "3  (T, h, a, t,  , d, o, e, s, n, ', t,  , s, o, ...  0.525000   \n",
       "4  (W, h, y,  , t, h, e,  , h, e, l, l,  , Y, o, ...  0.000000   \n",
       "\n",
       "                                         joined_stem  \n",
       "0  sooo you re on a destini reddit talk about fal...  \n",
       "1  `` hey , if littl joey 's dead , then i got no...  \n",
       "2                                     scandinavian ?  \n",
       "3      that doe n't sound veri support on his part .  \n",
       "4  whi the hell youtub result contain these video...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features\n",
    "chosen_data['joined_stem'] = chosen_data['stem'].apply(lambda x: ' '.join(x))\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(chosen_data[['joined_POS_tag', 'joined_stem']] , chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joined_POS_tag</th>\n",
       "      <th>joined_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7756</th>\n",
       "      <td>Is_VBZ this_DT the_DT Cable_JJ with_IN full_JJ...</td>\n",
       "      <td>is this the cabl with full use of his psionic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30313</th>\n",
       "      <td>I_PRP honestly_RB do_VBP n't_RB get_VB it_PRP !_.</td>\n",
       "      <td>i honest do n't get it !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11299</th>\n",
       "      <td>Has_NNP n't_RB closed_VBD yet_RB</td>\n",
       "      <td>has n't close yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25645</th>\n",
       "      <td>Yeah_UH ,_, the_DT people_NNS that_WDT are_VBP...</td>\n",
       "      <td>yeah , the peopl that are upset at have their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30009</th>\n",
       "      <td>u/junior312_NN is_VBZ the_DT girl_NN on_IN the...</td>\n",
       "      <td>u/junior312 is the girl on the right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          joined_POS_tag  \\\n",
       "7756   Is_VBZ this_DT the_DT Cable_JJ with_IN full_JJ...   \n",
       "30313  I_PRP honestly_RB do_VBP n't_RB get_VB it_PRP !_.   \n",
       "11299                   Has_NNP n't_RB closed_VBD yet_RB   \n",
       "25645  Yeah_UH ,_, the_DT people_NNS that_WDT are_VBP...   \n",
       "30009  u/junior312_NN is_VBZ the_DT girl_NN on_IN the...   \n",
       "\n",
       "                                             joined_stem  \n",
       "7756   is this the cabl with full use of his psionic ...  \n",
       "30313                           i honest do n't get it !  \n",
       "11299                                  has n't close yet  \n",
       "25645  yeah , the peopl that are upset at have their ...  \n",
       "30009               u/junior312 is the girl on the right  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF to vectorize the data\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stem_vectorizer = TfidfVectorizer(ngram_range = (1,3)) # ask about max features\n",
    "tfidf_stem = stem_vectorizer.fit_transform(X_train.joined_stem)\n",
    "\n",
    "pos_vectorizer = TfidfVectorizer(ngram_range = (1,3))\n",
    "tfidf_pos = pos_vectorizer.fit_transform(X_train.joined_POS_tag)\n",
    "\n",
    "combined_2 = sp.hstack([tfidf_stem, tfidf_pos], format = 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40264, 463819)\n",
      "(40264, 539072)\n",
      "(40264, 1002891)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_stem.shape)\n",
    "print(tfidf_pos.shape)\n",
    "print(combined_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 68926)\t0.17464625294469144\n",
      "  (0, 68961)\t0.2261560635942137\n",
      "  (0, 68962)\t0.2261560635942137\n",
      "  (0, 137219)\t0.14043789532410667\n",
      "  (0, 137389)\t0.2261560635942137\n",
      "  (0, 137390)\t0.2261560635942137\n",
      "  (0, 167961)\t0.100556058244647\n",
      "  (0, 168860)\t0.2261560635942137\n",
      "  (0, 168861)\t0.2261560635942137\n",
      "  (0, 186577)\t0.0602433756178877\n",
      "  (0, 191822)\t0.14527178836940863\n",
      "  (0, 191876)\t0.18901455125720107\n",
      "  (0, 260509)\t0.06322171593365364\n",
      "  (0, 262483)\t0.15399318462147402\n",
      "  (0, 262526)\t0.2261560635942137\n",
      "  (0, 294804)\t0.134557595290609\n",
      "  (0, 299969)\t0.21775114788222044\n",
      "  (0, 299970)\t0.2261560635942137\n",
      "  (0, 365115)\t0.04626843014667722\n",
      "  (0, 367063)\t0.21775114788222044\n",
      "  (0, 367065)\t0.2261560635942137\n",
      "  (0, 389500)\t0.0747193215890801\n",
      "  (0, 391474)\t0.1817944764623401\n",
      "  (0, 391477)\t0.2261560635942137\n",
      "  (0, 417015)\t0.10033925016189488\n",
      "  :\t:\n",
      "  (40262, 781821)\t0.15875025231707407\n",
      "  (40262, 832857)\t0.14140081028366594\n",
      "  (40262, 832862)\t0.17966043470029677\n",
      "  (40262, 832866)\t0.19613276671264013\n",
      "  (40262, 846341)\t0.11616094537052617\n",
      "  (40262, 846357)\t0.19613276671264013\n",
      "  (40262, 846358)\t0.19613276671264013\n",
      "  (40262, 874933)\t0.12544959271129907\n",
      "  (40262, 890605)\t0.06628476977505249\n",
      "  (40262, 891747)\t0.17361160958206828\n",
      "  (40262, 891752)\t0.19613276671264013\n",
      "  (40262, 923307)\t0.06513520948343961\n",
      "  (40262, 925219)\t0.13354985435585218\n",
      "  (40262, 925904)\t0.09223900978729954\n",
      "  (40262, 926613)\t0.18884364367794848\n",
      "  (40262, 926614)\t0.19613276671264013\n",
      "  (40262, 994048)\t0.05106023281294226\n",
      "  (40262, 997247)\t0.15072726455609806\n",
      "  (40262, 997260)\t0.19613276671264013\n",
      "  (40263, 15922)\t0.4624699449567666\n",
      "  (40263, 15933)\t0.6269455917428901\n",
      "  (40263, 150528)\t0.6269455917428901\n",
      "  (40263, 497526)\t0.5023572597275577\n",
      "  (40263, 497527)\t0.6114070590036637\n",
      "  (40263, 645679)\t0.6114070590036637\n"
     ]
    }
   ],
   "source": [
    "print(combined_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 50330, expected 40264.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-95aa544d3212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \"\"\"\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    584\u001b[0m                                                     \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrow_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                                                     got=A.shape[0]))\n\u001b[0;32m--> 586\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbcol_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 50330, expected 40264."
     ]
    }
   ],
   "source": [
    "combined_3 = sp.hstack((combined_2,np.array(chosen_data.polarity)[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_stem_vectorizer = TfidfVectorizer(max_features = 10000) # ask about max features\n",
    "test_tfidf_stem = stem_vectorizer.transform(X_test.joined_stem)\n",
    "\n",
    "# test_pos_vectorizer = TfidfVectorizer(max_features = 10000)\n",
    "test_tfidf_pos = pos_vectorizer.transform(X_test.joined_POS_tag)\n",
    "\n",
    "test_combined_2 = sp.hstack([test_tfidf_stem, test_tfidf_pos], format = 'csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4027, 454979)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_combined_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 300).fit(combined_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9802545793231916\n",
      "0.6776756890985846\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(combined_2, y_train))\n",
    "print(log_clf.score(test_combined_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9967711890717169\n",
      "0.6687360317854483\n"
     ]
    }
   ],
   "source": [
    "# SVM model\n",
    "svm_clf = LinearSVC()\n",
    "\n",
    "# Training the model\n",
    "svm_clf.fit(combined_2, y_train)\n",
    "\n",
    "print(svm_clf.score(combined_2, y_train))\n",
    "print(svm_clf.score(test_combined_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9970195591431232\n",
      "0.6530916314874596\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "rf_clf.fit(combined_2, y_train)\n",
    "\n",
    "print(rf_clf.score(combined_2, y_train))\n",
    "print(rf_clf.score(test_combined_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the features and labels for the models\n",
    "# features = chosen_data['stem'].apply(lambda x: ' '.join(x))\n",
    "# labels = chosen_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tfidf_train_x = vectorizer.fit_transform(X_train)\n",
    "# print(vectorizer.fit_transform(chosen_data['joined_stem']).toarray().shape)\n",
    "\n",
    "# The dimensions from the tfidf vectorization doesn't fit the dataframe\n",
    "# I'm not sure how to include both features in the model\n",
    "\n",
    "# chosen_data['tfidf_stem'] = vectorizer.fit_transform(chosen_data['joined_stem']).toarray()\n",
    "# chosen_data['tfidf_pos'] = vectorizer.fit_transform(chosen_data['joined_POS_tag']).toarray()\n",
    "# chosen_data.head()\n",
    "# features = list(features)\n",
    "# X = vectorizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vectorized the comments here and used the TF-IDF list as the feature in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = chosen_data[['tfidf_stem', 'tfidf_pos']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x1.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.get_feature_names())\n",
    "# print(X.shape)\n",
    "x2 = vectorizer.fit_transform(chosen_data['joined_POS_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x1)\n",
    "# print(x2)\n",
    "# print(x1.shape)\n",
    "# print(x2.shape)\n",
    "# print(x1.toarray())\n",
    "# print(list(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9fe0b356d478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# d = {'stem': x1.toarray(), 'pos': list(x2)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "# d = {'stem': x1.toarray(), 'pos': list(x2)}\n",
    "\n",
    "X = [x1, x2].as_matrix()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "estimators = [('tfidf', TfidfVectorizer())]\n",
    "combined = FeatureUnion(estimators)\n",
    "combined\n",
    "# features = chosen_data.iloc[:, 3:6:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joined_POS_tag</th>\n",
       "      <th>joined_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh_UH ,_, they_PRP do_VBP ._.</td>\n",
       "      <td>oh , they do .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>makes_VBZ it_PRP way_NN to_TO much_JJ red_JJ a...</td>\n",
       "      <td>make it way to much red and yellow imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Um_NN ..._: What_WP ?_.</td>\n",
       "      <td>um ... what ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For_IN select_JJ teams_NNS :_: 49ers_CD somewh...</td>\n",
       "      <td>for select team : 49er somewher in the top 4 r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If_IN you_PRP make_VBP coffee_NN often_RB you_...</td>\n",
       "      <td>if you make coffe often you alreadi have all t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      joined_POS_tag  \\\n",
       "0                      Oh_UH ,_, they_PRP do_VBP ._.   \n",
       "1  makes_VBZ it_PRP way_NN to_TO much_JJ red_JJ a...   \n",
       "2                            Um_NN ..._: What_WP ?_.   \n",
       "3  For_IN select_JJ teams_NNS :_: 49ers_CD somewh...   \n",
       "4  If_IN you_PRP make_VBP coffee_NN often_RB you_...   \n",
       "\n",
       "                                         joined_stem  \n",
       "0                                     oh , they do .  \n",
       "1             make it way to much red and yellow imo  \n",
       "2                                      um ... what ?  \n",
       "3  for select team : 49er somewher in the top 4 r...  \n",
       "4  if you make coffe often you alreadi have all t...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = chosen_data.iloc[:, 5:7]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.fit_transform(features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x2 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "X = combined.fit_transform(features)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)\n",
    "# X.toarray() # it only works if it's an array but why does it become all 0s?\n",
    "X = chosen_data.iloc[:, 3:6:2]\n",
    "X\n",
    "# chosen_data.iloc[:, 3:6:2]\n",
    "# print(chosen_data.iloc[0, 3])\n",
    "X['stem'] = X['stem'].apply(lambda x: x.toarray())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>noun_phrase_chunk</th>\n",
       "      <th>joined_POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[How, 's, his, wife, ?]</td>\n",
       "      <td>[(How, WRB), ('s, VBZ), (his, PRP$), (wife, NN...</td>\n",
       "      <td>[how, 's, his, wife, ?]</td>\n",
       "      <td>[(How, WRB), ('s, VBZ), (his, PRP$), [(wife, N...</td>\n",
       "      <td>How_WRB 's_VBZ his_PRP$ wife_NN ?_.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[And, let, Artemis, traps, act, like, springs,...</td>\n",
       "      <td>[(And, CC), (let, VB), (Artemis, NNP), (traps,...</td>\n",
       "      <td>[and, let, artemi, trap, act, like, spring, wh...</td>\n",
       "      <td>[(And, CC), (let, VB), (Artemis, NNP), (traps,...</td>\n",
       "      <td>And_CC let_VB Artemis_NNP traps_VB act_NN like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[my, first, thought, :, ``, Why, is, a, wookie...</td>\n",
       "      <td>[(my, PRP$), (first, JJ), (thought, NN), (:, :...</td>\n",
       "      <td>[my, first, thought, :, ``, whi, is, a, wooki,...</td>\n",
       "      <td>[(my, PRP$), [(first, JJ), (thought, NN)], (:,...</td>\n",
       "      <td>my_PRP$ first_JJ thought_NN :_: ``_`` Why_WRB ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[In, my, experience, the, people, who, make, t...</td>\n",
       "      <td>[(In, IN), (my, PRP$), (experience, NN), (the,...</td>\n",
       "      <td>[in, my, experi, the, peopl, who, make, those,...</td>\n",
       "      <td>[(In, IN), (my, PRP$), [(experience, NN)], (th...</td>\n",
       "      <td>In_IN my_PRP$ experience_NN the_DT people_NNS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Could, n't, find, it, I, guess, ?]</td>\n",
       "      <td>[(Could, MD), (n't, RB), (find, VB), (it, PRP)...</td>\n",
       "      <td>[could, n't, find, it, i, guess, ?]</td>\n",
       "      <td>[(Could, MD), (n't, RB), (find, VB), (it, PRP)...</td>\n",
       "      <td>Could_MD n't_RB find_VB it_PRP I_PRP guess_RB ?_.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0                            [How, 's, his, wife, ?]   \n",
       "1      0  [And, let, Artemis, traps, act, like, springs,...   \n",
       "2      0  [my, first, thought, :, ``, Why, is, a, wookie...   \n",
       "3      0  [In, my, experience, the, people, who, make, t...   \n",
       "4      0                [Could, n't, find, it, I, guess, ?]   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(How, WRB), ('s, VBZ), (his, PRP$), (wife, NN...   \n",
       "1  [(And, CC), (let, VB), (Artemis, NNP), (traps,...   \n",
       "2  [(my, PRP$), (first, JJ), (thought, NN), (:, :...   \n",
       "3  [(In, IN), (my, PRP$), (experience, NN), (the,...   \n",
       "4  [(Could, MD), (n't, RB), (find, VB), (it, PRP)...   \n",
       "\n",
       "                                                stem  \\\n",
       "0                            [how, 's, his, wife, ?]   \n",
       "1  [and, let, artemi, trap, act, like, spring, wh...   \n",
       "2  [my, first, thought, :, ``, whi, is, a, wooki,...   \n",
       "3  [in, my, experi, the, peopl, who, make, those,...   \n",
       "4                [could, n't, find, it, i, guess, ?]   \n",
       "\n",
       "                                   noun_phrase_chunk  \\\n",
       "0  [(How, WRB), ('s, VBZ), (his, PRP$), [(wife, N...   \n",
       "1  [(And, CC), (let, VB), (Artemis, NNP), (traps,...   \n",
       "2  [(my, PRP$), [(first, JJ), (thought, NN)], (:,...   \n",
       "3  [(In, IN), (my, PRP$), [(experience, NN)], (th...   \n",
       "4  [(Could, MD), (n't, RB), (find, VB), (it, PRP)...   \n",
       "\n",
       "                                      joined_POS_tag  \n",
       "0                How_WRB 's_VBZ his_PRP$ wife_NN ?_.  \n",
       "1  And_CC let_VB Artemis_NNP traps_VB act_NN like...  \n",
       "2  my_PRP$ first_JJ thought_NN :_: ``_`` Why_WRB ...  \n",
       "3  In_IN my_PRP$ experience_NN the_DT people_NNS ...  \n",
       "4  Could_MD n't_RB find_VB it_PRP I_PRP guess_RB ?_.  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test values\n",
    "\n",
    "log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5003711952487008\n",
      "0.49851632047477745\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(X_train, y_train))\n",
    "print(log_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "\n",
    "svm_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5003711952487008\n",
      "0.49851632047477745\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.score(X_train, y_train))\n",
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:  0.5003711952487008\n",
      "test score:  0.5024727992087042\n"
     ]
    }
   ],
   "source": [
    "print(\"training score: \", rf_clf.score(X_train, y_train))\n",
    "print(\"test score: \", rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = chosen_data['joined_POS_tag']\n",
    "#features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = list(features2)\n",
    "X2 = vectorizer.fit_transform(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<151614x95627 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1564163 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-638289c5a63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X2' is not defined"
     ]
    }
   ],
   "source": [
    "print(X2, \"\\n\")\n",
    "print(X2.toarray(), \"\\n\")\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7819541433412207\n",
      "0.6916861788081654\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(X_train, y_train))\n",
    "print(log_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8720515124782547\n",
      "0.6900702437093955\n"
     ]
    }
   ],
   "source": [
    "# SVM model\n",
    "svm_clf = LinearSVC()\n",
    "\n",
    "# Training the model\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "print(svm_clf.score(X_train, y_train))\n",
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980196387201029\n",
      "0.6772746759885235\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 10)\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "print(rf_clf.score(X_train, y_train))\n",
    "print(rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

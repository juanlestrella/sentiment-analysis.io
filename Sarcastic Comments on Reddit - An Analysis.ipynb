{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nour Mansour and Juan Estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Data Collection\n",
    "\n",
    "Step 1: Get the tsv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data is saved in the same folder as the project. Then read data from tsv file\n",
    "data = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
    "data.dropna(inplace=True)\n",
    "data.drop(['author', 'score', 'ups', 'downs', 'date', 'created_utc', 'subreddit', 'parent_comment'], axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Data Processing\n",
    "\n",
    "Step 1: Columns required: Label, Comments, subreddit, parent comment\n",
    "\n",
    "Step 2: Create a Dataframe containing an even amount of sarcastic and non sarcastic \n",
    "comments, amount of data is 505413 each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010768</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure that Iran and N. Korea have the techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010769</th>\n",
       "      <td>1</td>\n",
       "      <td>whatever you do, don't vote green!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010770</th>\n",
       "      <td>1</td>\n",
       "      <td>Perhaps this is an atheist conspiracy to make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010771</th>\n",
       "      <td>1</td>\n",
       "      <td>The Slavs got their own country - it is called...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010772</th>\n",
       "      <td>1</td>\n",
       "      <td>values, as in capitalism .. there is good mone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment\n",
       "1010768      1  I'm sure that Iran and N. Korea have the techn...\n",
       "1010769      1                 whatever you do, don't vote green!\n",
       "1010770      1  Perhaps this is an atheist conspiracy to make ...\n",
       "1010771      1  The Slavs got their own country - it is called...\n",
       "1010772      1  values, as in capitalism .. there is good mone..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Copy the only the data we need\n",
    "#required_data = data[['label','comment','subreddit','parent_comment']]\n",
    "#required_data.head()\n",
    "# Reset the indices after rows with NA values are dropped\n",
    "data.reset_index(inplace = True)\n",
    "data.drop(['index'], axis = 1, inplace = True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>198103</th>\n",
       "      <td>0</td>\n",
       "      <td>Hitsugaya being in every fight is partly becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546828</th>\n",
       "      <td>0</td>\n",
       "      <td>I imagine his nickname in prison would be 'The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989843</th>\n",
       "      <td>0</td>\n",
       "      <td>what about the part where its solo queue :/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7539</th>\n",
       "      <td>0</td>\n",
       "      <td>Yea great example, I see Gold players pull off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890177</th>\n",
       "      <td>0</td>\n",
       "      <td>Well it must've been a glitch then, because I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                            comment\n",
       "0 198103      0  Hitsugaya being in every fight is partly becau...\n",
       "  546828      0  I imagine his nickname in prison would be 'The...\n",
       "  989843      0        what about the part where its solo queue :/\n",
       "  7539        0  Yea great example, I see Gold players pull off...\n",
       "  890177      0  Well it must've been a glitch then, because I ..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rand_ind = np.random.choice(data.index, int(len(data)*10/100))\n",
    "#len(rand_ind) # 101077 for 10%\n",
    "\n",
    "#chosen_data = data.sample(frac=0.5, random_state=1)\n",
    "#chosen_data.tail()\n",
    "\n",
    "\n",
    "\n",
    "size = int(len(data)*5/100)# sample size\n",
    "replace = True  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "chosen_data = data.groupby('label', as_index=False).apply(fn)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hitsugaya being in every fight is partly becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I imagine his nickname in prison would be 'The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>what about the part where its solo queue :/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Yea great example, I see Gold players pull off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Well it must've been a glitch then, because I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0  Hitsugaya being in every fight is partly becau...\n",
       "1      0  I imagine his nickname in prison would be 'The...\n",
       "2      0        what about the part where its solo queue :/\n",
       "3      0  Yea great example, I see Gold players pull off...\n",
       "4      0  Well it must've been a glitch then, because I ..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.reset_index(inplace = True)\n",
    "chosen_data.drop(['level_0', 'level_1'], inplace = True, axis = 1)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101071</th>\n",
       "      <td>1</td>\n",
       "      <td>Number 13 will blow your mind!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101072</th>\n",
       "      <td>1</td>\n",
       "      <td>Surprised to hear them featured in the NME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101073</th>\n",
       "      <td>1</td>\n",
       "      <td>Stay classy reddit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101074</th>\n",
       "      <td>1</td>\n",
       "      <td>Random Beta invites for viewers would be a start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101075</th>\n",
       "      <td>1</td>\n",
       "      <td>the new robbie schremp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                           comment\n",
       "101071      1                    Number 13 will blow your mind!\n",
       "101072      1        Surprised to hear them featured in the NME\n",
       "101073      1                               Stay classy reddit.\n",
       "101074      1  Random Beta invites for viewers would be a start\n",
       "101075      1                            the new robbie schremp"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Exploratory Analysis & Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore distribution of the data by label (0 -> non-sarcastic, 1 -> sarcastic)\n",
    "plt.hist(chosen_data.label)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Comments by Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50538</td>\n",
       "      <td>46926</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50538</td>\n",
       "      <td>47165</td>\n",
       "      <td>You forgot the</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment                            \n",
       "        count unique             top freq\n",
       "label                                    \n",
       "0       50538  46926             Yes   38\n",
       "1       50538  47165  You forgot the  138"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore distribution of comments by label \n",
    "chosen_data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "#Import NLTK library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #Use this to get rid off meaningless words like \"the, and, a\"\n",
    "from nltk.tokenize import word_tokenize #Split by word\n",
    "from nltk.tokenize import sent_tokenize #Split by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all the comment column is str data type\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "\n",
    "#non_sarcastic = required_data.loc[required_data['label'] == 0]\n",
    "#sarcastic = required_data.loc[required_data['label'] == 1]\n",
    "\n",
    "#drop rows with na values on the comment column\n",
    "#non_sarcastic['comment'].dropna(inplace=True)\n",
    "#sarcastic['comment'].dropna(inplace=True)\n",
    "\n",
    "#Make sure all the comment column is str data type\n",
    "#non_sarcastic['comment'] = non_sarcastic['comment'].astype(str)\n",
    "#sarcastic['comment'] = sarcastic['comment'].astype(str)\n",
    "#print(len(non_sarcastic), len(sarcastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...  \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...  \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...  \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...  \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data['comment'] = chosen_data['comment'].apply(word_tokenize)\n",
    "chosen_data['POS_tag'] = chosen_data['comment'].apply(nltk.pos_tag)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "      <td>[hitsugaya, be, in, everi, fight, is, part, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "      <td>[i, imagin, his, nicknam, in, prison, would, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "      <td>[what, about, the, part, where, it, solo, queu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "      <td>[yea, great, exampl, ,, i, see, gold, player, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>[well, it, must, ve, been, a, glitch, then, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...   \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...   \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...   \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...   \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "\n",
       "                                                stem  \n",
       "0  [hitsugaya, be, in, everi, fight, is, part, be...  \n",
       "1  [i, imagin, his, nicknam, in, prison, would, b...  \n",
       "2  [what, about, the, part, where, it, solo, queu...  \n",
       "3  [yea, great, exampl, ,, i, see, gold, player, ...  \n",
       "4  [well, it, must, ve, been, a, glitch, then, ,,...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "chosen_data['stem'] = chosen_data['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_len = len(data)\n",
    "#data1 = data.iloc[0:(int(data_len/4))].copy()\n",
    "#data1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 = data.iloc[(int(data_len/4)):(int(data_len/2))].copy()\n",
    "#data2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3 = data.iloc[(int(data_len/2)):(int(3*data_len/4))].copy()\n",
    "#data3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data4 = data.iloc[(int(3*data_len/4)):].copy()\n",
    "#data4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['comment'] = data4['comment'].apply(word_tokenize)\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['POS_tag'] = data4['comment'].apply(nltk.pos_tag)\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['stem'] = data4['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data3['comment'] = data3['comment'].apply(word_tokenize)\n",
    "# data3['POS_tag'] = data3['comment'].apply(nltk.pos_tag)\n",
    "# data3['stem'] = data3['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2['comment'] = data2['comment'].apply(word_tokenize)\n",
    "# data2['POS_tag'] = data2['comment'].apply(nltk.pos_tag)\n",
    "# data2['stem'] = data2['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1['comment'] = data1['comment'].apply(word_tokenize)\n",
    "# data1['POS_tag'] = data1['comment'].apply(nltk.pos_tag)\n",
    "# data1['stem'] = data1['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the required data set in half to ease later comment transformations\n",
    "# df1 = data.iloc[0:(int(len(data)/2))].copy()\n",
    "# df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = data.iloc[(int(len(data)/2)):].copy()\n",
    "# df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['tokenized_by_sent'] = df1['comment'].apply(sent_tokenize)\n",
    "#df2['tokenized_by_sent'] = df2['comment'].apply(sent_tokenize)\n",
    "\n",
    "#data['tokenized_by_sent'] = data['comment'].apply(sent_tokenize)\n",
    "#data['tokenized_by_word'] = data['comment'].apply(word_tokenize)\n",
    "#word = word_tokenize(sarcastic['comment'].iloc[56269])\n",
    "#word\n",
    "#note to nour, so when i try to do the first line of code it shows that index 56269 has an error but when i try to \n",
    "#individually tokenized that index, it works. Not sure how to fix this.\n",
    "\n",
    "# note to Juan: Fixed by making dropna function is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by word first half\n",
    "\n",
    "# df1['comment'] = df1['comment'].apply(word_tokenize)\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by word second half\n",
    "\n",
    "# df2['comment'] = df2['comment'].apply(word_tokenize)\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of Speech tagging first half\n",
    "\n",
    "# df1['POS_tag'] = df1['comment'].apply(nltk.pos_tag)\n",
    "# df1.head()\n",
    "\n",
    "#data['POS_tag'] = data['tokenized_by_word'].apply(nltk.pos_tag)\n",
    "#nltk.pos_tag(data.iloc[0, 5])\n",
    "#entities = nltk.chunk.ne_chunk( nltk.pos_tag(data.iloc[1, 5]))\n",
    "#entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of Speech tagging second half\n",
    "\n",
    "# df2['POS_tag'] = df2['comment'].apply(nltk.pos_tag)\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()\n",
    "# df1_len = len(df1)\n",
    "# df2_len = len(df2)\n",
    "# df14 = df1.iloc[0:int(df1_len/2)].copy()\n",
    "# df24 = df1.iloc[(int(df1_len/2)):].copy()\n",
    "# df34 = df2.iloc[0:int(df2_len/2)].copy()\n",
    "# df44 = df2.iloc[(int(df2_len/2)):].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "#from nltk.stem import PorterStemmer\n",
    "\n",
    "#data['stem'] = data['tokenized_by_word'].apply(lambda x: [PorterStemmer.stem(y) for y in x])\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# df14['stem'] = df14['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df14.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df24['stem'] = df24['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df24.head()\n",
    "#df2['stem'] = df2['tokenized_by_word'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "#df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df34['stem'] = df34['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df34.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df44['stem'] = df44['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df44.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import WordNetLemmatizer\n",
    "#data['lemma'] = data['tokenized_by_word'].apply(WordNetLemmatizer)\n",
    "# df14.append(df24.append(df34.append(df44)))\n",
    "# df14.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Sarcasm\n",
    "\n",
    "We were interested in trying different models for predicting a sarcastic comment.\n",
    "To determine the features and labels for the analysis, we decided to look at the comment itself as a feature and use the given sarcastic vs non-sarcastic classification as our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the features and labels for the models\n",
    "features = chosen_data['stem'].apply(lambda x: ' '.join(x))\n",
    "labels = chosen_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF to vectorize the data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer() # ask about max features\n",
    "features = list(features)\n",
    "X = vectorizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101076, 36655)\n"
     ]
    }
   ],
   "source": [
    "#print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<101076x36655 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 951727 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(X)\n",
    "X.toarray() # it only works if it's an array but why does it become all 0s?\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "      <td>[hitsugaya, be, in, everi, fight, is, part, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "      <td>[i, imagin, his, nicknam, in, prison, would, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "      <td>[what, about, the, part, where, it, solo, queu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "      <td>[yea, great, exampl, ,, i, see, gold, player, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>[well, it, must, ve, been, a, glitch, then, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...   \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...   \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...   \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...   \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "\n",
       "                                                stem  \n",
       "0  [hitsugaya, be, in, everi, fight, is, part, be...  \n",
       "1  [i, imagin, his, nicknam, in, prison, would, b...  \n",
       "2  [what, about, the, part, where, it, solo, queu...  \n",
       "3  [yea, great, exampl, ,, i, see, gold, player, ...  \n",
       "4  [well, it, must, ve, been, a, glitch, then, ,,...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation?\n",
    "\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 200).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test values\n",
    "\n",
    "log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7479099678456592\n",
      "0.6768401266323704\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(X_train, y_train))\n",
    "print(log_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "\n",
    "svm_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807308929013109\n",
      "0.6664523149980214\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.score(X_train, y_train))\n",
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9792233489982686\n",
      "0.6721408785120696\n"
     ]
    }
   ],
   "source": [
    "print(rf_clf.score(X_train, y_train))\n",
    "print(rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunking = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = nltk.RegexpParser(np_chunking)\n",
    "chosen_data['noun_phrase_chunk'] = chosen_data['POS_tag'].apply(chunk_parser.parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>noun_phrase_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "      <td>[hitsugaya, be, in, everi, fight, is, part, be...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), [(e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "      <td>[i, imagin, his, nicknam, in, prison, would, b...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), [(nick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "      <td>[what, about, the, part, where, it, solo, queu...</td>\n",
       "      <td>[(what, WP), (about, IN), [(the, DT), (part, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "      <td>[yea, great, exampl, ,, i, see, gold, player, ...</td>\n",
       "      <td>[(Yea, NNP), [(great, JJ), (example, NN)], (,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>[well, it, must, ve, been, a, glitch, then, ,,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...   \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...   \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...   \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...   \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [hitsugaya, be, in, everi, fight, is, part, be...   \n",
       "1  [i, imagin, his, nicknam, in, prison, would, b...   \n",
       "2  [what, about, the, part, where, it, solo, queu...   \n",
       "3  [yea, great, exampl, ,, i, see, gold, player, ...   \n",
       "4  [well, it, must, ve, been, a, glitch, then, ,,...   \n",
       "\n",
       "                                   noun_phrase_chunk  \n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), [(e...  \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), [(nick...  \n",
       "2  [(what, WP), (about, IN), [(the, DT), (part, N...  \n",
       "3  [(Yea, NNP), [(great, JJ), (example, NN)], (,,...  \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>noun_phrase_chunk</th>\n",
       "      <th>joined_POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "      <td>[hitsugaya, be, in, everi, fight, is, part, be...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), [(e...</td>\n",
       "      <td>Hitsugaya_NNP being_VBG in_IN every_DT fight_N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "      <td>[i, imagin, his, nicknam, in, prison, would, b...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), [(nick...</td>\n",
       "      <td>I_PRP imagine_VBP his_PRP$ nickname_NN in_IN p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "      <td>[what, about, the, part, where, it, solo, queu...</td>\n",
       "      <td>[(what, WP), (about, IN), [(the, DT), (part, N...</td>\n",
       "      <td>what_WP about_IN the_DT part_NN where_WRB its_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "      <td>[yea, great, exampl, ,, i, see, gold, player, ...</td>\n",
       "      <td>[(Yea, NNP), [(great, JJ), (example, NN)], (,,...</td>\n",
       "      <td>Yea_NNP great_JJ example_NN ,_, I_PRP see_VBP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>[well, it, must, ve, been, a, glitch, then, ,,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>Well_IN it_PRP must_MD 've_VBP been_VBN a_DT g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[If, Bolvar, was, 6/7, for, 5, mana, (, silenc...</td>\n",
       "      <td>[(If, IN), (Bolvar, NNP), (was, VBD), (6/7, CD...</td>\n",
       "      <td>[if, bolvar, was, 6/7, for, 5, mana, (, silenc...</td>\n",
       "      <td>[(If, IN), (Bolvar, NNP), (was, VBD), (6/7, CD...</td>\n",
       "      <td>If_IN Bolvar_NNP was_VBD 6/7_CD for_IN 5_CD ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[BACON]</td>\n",
       "      <td>[(BACON, NN)]</td>\n",
       "      <td>[bacon]</td>\n",
       "      <td>[[(BACON, NN)]]</td>\n",
       "      <td>BACON_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[Sounds, like, a, Kmart, customer, .]</td>\n",
       "      <td>[(Sounds, NNS), (like, IN), (a, DT), (Kmart, N...</td>\n",
       "      <td>[sound, like, a, kmart, custom, .]</td>\n",
       "      <td>[(Sounds, NNS), (like, IN), (a, DT), (Kmart, N...</td>\n",
       "      <td>Sounds_NNS like_IN a_DT Kmart_NNP customer_NN ._.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[Windows, 7, ,, but, you, either, need, to, ta...</td>\n",
       "      <td>[(Windows, NNS), (7, CD), (,, ,), (but, CC), (...</td>\n",
       "      <td>[window, 7, ,, but, you, either, need, to, tak...</td>\n",
       "      <td>[(Windows, NNS), (7, CD), (,, ,), (but, CC), (...</td>\n",
       "      <td>Windows_NNS 7_CD ,_, but_CC you_PRP either_RB ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[Anytime, I, 've, see, this, guys, videos, I, ...</td>\n",
       "      <td>[(Anytime, NNP), (I, PRP), ('ve, VBP), (see, V...</td>\n",
       "      <td>[anytim, i, ve, see, this, guy, video, i, want...</td>\n",
       "      <td>[(Anytime, NNP), (I, PRP), ('ve, VBP), (see, V...</td>\n",
       "      <td>Anytime_NNP I_PRP 've_VBP see_VB this_DT guys_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, think, thats, what, all, gas, outlets, for...</td>\n",
       "      <td>[(I, PRP), (think, VBP), (thats, NNS), (what, ...</td>\n",
       "      <td>[i, think, that, what, all, gas, outlet, for, ...</td>\n",
       "      <td>[(I, PRP), (think, VBP), (thats, NNS), (what, ...</td>\n",
       "      <td>I_PRP think_VBP thats_NNS what_WP all_DT gas_N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>[You, use, the, word, ``, retarded, '', a, lot...</td>\n",
       "      <td>[(You, PRP), (use, VBP), (the, DT), (word, NN)...</td>\n",
       "      <td>[you, use, the, word, ``, retard, '', a, lot, .]</td>\n",
       "      <td>[(You, PRP), (use, VBP), [(the, DT), (word, NN...</td>\n",
       "      <td>You_PRP use_VBP the_DT word_NN ``_`` retarded_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, still, want, him, to, start, doing, khaki,...</td>\n",
       "      <td>[(I, PRP), (still, RB), (want, VBP), (him, PRP...</td>\n",
       "      <td>[i, still, want, him, to, start, do, khaki, co...</td>\n",
       "      <td>[(I, PRP), (still, RB), (want, VBP), (him, PRP...</td>\n",
       "      <td>I_PRP still_RB want_VBP him_PRP to_TO start_VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[What, about, the, bodily, autonomy, lost, whe...</td>\n",
       "      <td>[(What, WP), (about, IN), (the, DT), (bodily, ...</td>\n",
       "      <td>[what, about, the, bodili, autonomi, lost, whe...</td>\n",
       "      <td>[(What, WP), (about, IN), [(the, DT), (bodily,...</td>\n",
       "      <td>What_WP about_IN the_DT bodily_JJ autonomy_NN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[So, who, exactly, qualifies, as, a, minority, ?]</td>\n",
       "      <td>[(So, RB), (who, WP), (exactly, RB), (qualifie...</td>\n",
       "      <td>[so, who, exact, qualifi, as, a, minor, ?]</td>\n",
       "      <td>[(So, RB), (who, WP), (exactly, RB), (qualifie...</td>\n",
       "      <td>So_RB who_WP exactly_RB qualifies_VBZ as_IN a_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[Tire, felt, cramped, and, gtfo, 'd]</td>\n",
       "      <td>[(Tire, NNP), (felt, VBD), (cramped, VBN), (an...</td>\n",
       "      <td>[tire, felt, cramp, and, gtfo, 'd]</td>\n",
       "      <td>[(Tire, NNP), (felt, VBD), (cramped, VBN), (an...</td>\n",
       "      <td>Tire_NNP felt_VBD cramped_VBN and_CC gtfo_JJ '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, knew, I, 'd, get, a, kick, out, of, postin...</td>\n",
       "      <td>[(I, PRP), (knew, VBD), (I, PRP), ('d, MD), (g...</td>\n",
       "      <td>[i, knew, i, 'd, get, a, kick, out, of, post, ...</td>\n",
       "      <td>[(I, PRP), (knew, VBD), (I, PRP), ('d, MD), (g...</td>\n",
       "      <td>I_PRP knew_VBD I_PRP 'd_MD get_VB a_DT kick_NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>[Off, course, !]</td>\n",
       "      <td>[(Off, IN), (course, NN), (!, .)]</td>\n",
       "      <td>[off, cours, !]</td>\n",
       "      <td>[(Off, IN), [(course, NN)], (!, .)]</td>\n",
       "      <td>Off_IN course_NN !_.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ramen, ?]</td>\n",
       "      <td>[(Ramen, NNS), (?, .)]</td>\n",
       "      <td>[ramen, ?]</td>\n",
       "      <td>[(Ramen, NNS), (?, .)]</td>\n",
       "      <td>Ramen_NNS ?_.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[Games, would, look, retro, XD]</td>\n",
       "      <td>[(Games, NNS), (would, MD), (look, VB), (retro...</td>\n",
       "      <td>[game, would, look, retro, xd]</td>\n",
       "      <td>[(Games, NNS), (would, MD), (look, VB), [(retr...</td>\n",
       "      <td>Games_NNS would_MD look_VB retro_JJ XD_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>[My, mom, got, me, a, Dooney, and, Burke, Navy...</td>\n",
       "      <td>[(My, PRP$), (mom, NN), (got, VBD), (me, PRP),...</td>\n",
       "      <td>[my, mom, got, me, a, dooney, and, burk, navi,...</td>\n",
       "      <td>[(My, PRP$), [(mom, NN)], (got, VBD), (me, PRP...</td>\n",
       "      <td>My_PRP$ mom_NN got_VBD me_PRP a_DT Dooney_NNP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>[If, you, work, in, IT, you, 're, gon, na, mak...</td>\n",
       "      <td>[(If, IN), (you, PRP), (work, VBP), (in, IN), ...</td>\n",
       "      <td>[if, you, work, in, it, you, re, gon, na, make...</td>\n",
       "      <td>[(If, IN), (you, PRP), (work, VBP), (in, IN), ...</td>\n",
       "      <td>If_IN you_PRP work_VBP in_IN IT_NNP you_PRP 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>[The, Half-Blood, Prince, of, rock]</td>\n",
       "      <td>[(The, DT), (Half-Blood, NNP), (Prince, NNP), ...</td>\n",
       "      <td>[the, half-blood, princ, of, rock]</td>\n",
       "      <td>[(The, DT), (Half-Blood, NNP), (Prince, NNP), ...</td>\n",
       "      <td>The_DT Half-Blood_NNP Prince_NNP of_IN rock_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>[his, work, defensively, is, really, great, ,,...</td>\n",
       "      <td>[(his, PRP$), (work, NN), (defensively, RB), (...</td>\n",
       "      <td>[his, work, defens, is, realli, great, ,, he, ...</td>\n",
       "      <td>[(his, PRP$), [(work, NN)], (defensively, RB),...</td>\n",
       "      <td>his_PRP$ work_NN defensively_RB is_VBZ really_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>[My, BAC]</td>\n",
       "      <td>[(My, PRP$), (BAC, NN)]</td>\n",
       "      <td>[my, bac]</td>\n",
       "      <td>[(My, PRP$), [(BAC, NN)]]</td>\n",
       "      <td>My_PRP$ BAC_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>[A, few, T-90, 's, and, volunteers, would, n't...</td>\n",
       "      <td>[(A, DT), (few, JJ), (T-90, NNP), ('s, POS), (...</td>\n",
       "      <td>[a, few, t-90, 's, and, volunt, would, n't, hu...</td>\n",
       "      <td>[(A, DT), (few, JJ), (T-90, NNP), ('s, POS), (...</td>\n",
       "      <td>A_DT few_JJ T-90_NNP 's_POS and_CC volunteers_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ew]</td>\n",
       "      <td>[(Ew, NN)]</td>\n",
       "      <td>[ew]</td>\n",
       "      <td>[[(Ew, NN)]]</td>\n",
       "      <td>Ew_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>[Wonder, if, he, 's, wishing, the, club, bit, ...</td>\n",
       "      <td>[(Wonder, VB), (if, IN), (he, PRP), ('s, VBZ),...</td>\n",
       "      <td>[wonder, if, he, 's, wish, the, club, bit, on,...</td>\n",
       "      <td>[(Wonder, VB), (if, IN), (he, PRP), ('s, VBZ),...</td>\n",
       "      <td>Wonder_VB if_IN he_PRP 's_VBZ wishing_VBG the_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>[PSN, was, down, last, night, too, ,, ended, u...</td>\n",
       "      <td>[(PSN, NN), (was, VBD), (down, RB), (last, JJ)...</td>\n",
       "      <td>[psn, was, down, last, night, too, ,, end, up,...</td>\n",
       "      <td>[[(PSN, NN)], (was, VBD), (down, RB), [(last, ...</td>\n",
       "      <td>PSN_NN was_VBD down_RB last_JJ night_NN too_RB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>[``, Two, men, brutally, assaulted, and, unlaw...</td>\n",
       "      <td>[(``, ``), (Two, CD), (men, NNS), (brutally, R...</td>\n",
       "      <td>[``, two, men, brutal, assault, and, unlaw, de...</td>\n",
       "      <td>[(``, ``), (Two, CD), (men, NNS), (brutally, R...</td>\n",
       "      <td>``_`` Two_CD men_NNS brutally_RB assaulted_VBD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101046</th>\n",
       "      <td>1</td>\n",
       "      <td>[Look, out, Teddy]</td>\n",
       "      <td>[(Look, VB), (out, RP), (Teddy, NNP)]</td>\n",
       "      <td>[look, out, teddi]</td>\n",
       "      <td>[(Look, VB), (out, RP), (Teddy, NNP)]</td>\n",
       "      <td>Look_VB out_RP Teddy_NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101047</th>\n",
       "      <td>1</td>\n",
       "      <td>[I, feel, so, much, safer, now, that, they, ha...</td>\n",
       "      <td>[(I, PRP), (feel, VBP), (so, RB), (much, JJ), ...</td>\n",
       "      <td>[i, feel, so, much, safer, now, that, they, ha...</td>\n",
       "      <td>[(I, PRP), (feel, VBP), (so, RB), [(much, JJ),...</td>\n",
       "      <td>I_PRP feel_VBP so_RB much_JJ safer_NN now_RB t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101048</th>\n",
       "      <td>1</td>\n",
       "      <td>[OMG, WAS, IT, IN, PORTLAND, ?]</td>\n",
       "      <td>[(OMG, NNP), (WAS, NNP), (IT, NNP), (IN, NNP),...</td>\n",
       "      <td>[omg, was, it, in, portland, ?]</td>\n",
       "      <td>[(OMG, NNP), (WAS, NNP), (IT, NNP), (IN, NNP),...</td>\n",
       "      <td>OMG_NNP WAS_NNP IT_NNP IN_NNP PORTLAND_NNP ?_.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101049</th>\n",
       "      <td>1</td>\n",
       "      <td>[But, teachers, unions, are, the, devil, ,, yo...</td>\n",
       "      <td>[(But, CC), (teachers, NNS), (unions, NNS), (a...</td>\n",
       "      <td>[but, teacher, union, are, the, devil, ,, you,...</td>\n",
       "      <td>[(But, CC), (teachers, NNS), (unions, NNS), (a...</td>\n",
       "      <td>But_CC teachers_NNS unions_NNS are_VBP the_DT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101050</th>\n",
       "      <td>1</td>\n",
       "      <td>[Windows, 10, literally, perfect, now..]</td>\n",
       "      <td>[(Windows, NNS), (10, CD), (literally, RB), (p...</td>\n",
       "      <td>[window, 10, liter, perfect, now..]</td>\n",
       "      <td>[(Windows, NNS), (10, CD), (literally, RB), [(...</td>\n",
       "      <td>Windows_NNS 10_CD literally_RB perfect_JJ now....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101051</th>\n",
       "      <td>1</td>\n",
       "      <td>[Slightly, change, the, color, scheme, !]</td>\n",
       "      <td>[(Slightly, RB), (change, VBP), (the, DT), (co...</td>\n",
       "      <td>[slight, chang, the, color, scheme, !]</td>\n",
       "      <td>[(Slightly, RB), (change, VBP), [(the, DT), (c...</td>\n",
       "      <td>Slightly_RB change_VBP the_DT color_NN scheme_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101052</th>\n",
       "      <td>1</td>\n",
       "      <td>[Whoa, whoa, whoa, there, HITLER, MUSSOLINI, T...</td>\n",
       "      <td>[(Whoa, NNP), (whoa, NN), (whoa, NN), (there, ...</td>\n",
       "      <td>[whoa, whoa, whoa, there, hitler, mussolini, t...</td>\n",
       "      <td>[(Whoa, NNP), [(whoa, NN)], [(whoa, NN)], (the...</td>\n",
       "      <td>Whoa_NNP whoa_NN whoa_NN there_RB HITLER_NNP M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101053</th>\n",
       "      <td>1</td>\n",
       "      <td>[And, a, woman, could, n't, get, fired, for, w...</td>\n",
       "      <td>[(And, CC), (a, DT), (woman, NN), (could, MD),...</td>\n",
       "      <td>[and, a, woman, could, n't, get, fire, for, wh...</td>\n",
       "      <td>[(And, CC), [(a, DT), (woman, NN)], (could, MD...</td>\n",
       "      <td>And_CC a_DT woman_NN could_MD n't_RB get_VB fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101054</th>\n",
       "      <td>1</td>\n",
       "      <td>[Leave, him, alone, he, is, clearly, trans-rac...</td>\n",
       "      <td>[(Leave, NNP), (him, PRP), (alone, RB), (he, P...</td>\n",
       "      <td>[leav, him, alon, he, is, clear, trans-rac, ,,...</td>\n",
       "      <td>[(Leave, NNP), (him, PRP), (alone, RB), (he, P...</td>\n",
       "      <td>Leave_NNP him_PRP alone_RB he_PRP is_VBZ clear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101055</th>\n",
       "      <td>1</td>\n",
       "      <td>[I, 'm, sure, the, developers, ,, artists, ,, ...</td>\n",
       "      <td>[(I, PRP), ('m, VBP), (sure, JJ), (the, DT), (...</td>\n",
       "      <td>[i, 'm, sure, the, develop, ,, artist, ,, and,...</td>\n",
       "      <td>[(I, PRP), ('m, VBP), (sure, JJ), (the, DT), (...</td>\n",
       "      <td>I_PRP 'm_VBP sure_JJ the_DT developers_NNS ,_,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101056</th>\n",
       "      <td>1</td>\n",
       "      <td>[Clearly, as, being, OP, is, a, VS, trait, and...</td>\n",
       "      <td>[(Clearly, RB), (as, IN), (being, VBG), (OP, N...</td>\n",
       "      <td>[clear, as, be, op, is, a, vs, trait, and, und...</td>\n",
       "      <td>[(Clearly, RB), (as, IN), (being, VBG), (OP, N...</td>\n",
       "      <td>Clearly_RB as_IN being_VBG OP_NNP is_VBZ a_DT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101057</th>\n",
       "      <td>1</td>\n",
       "      <td>[You, did, n't, include, a, picture, of, gay, ...</td>\n",
       "      <td>[(You, PRP), (did, VBD), (n't, RB), (include, ...</td>\n",
       "      <td>[you, did, n't, includ, a, pictur, of, gay, pe...</td>\n",
       "      <td>[(You, PRP), (did, VBD), (n't, RB), (include, ...</td>\n",
       "      <td>You_PRP did_VBD n't_RB include_VB a_DT picture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101058</th>\n",
       "      <td>1</td>\n",
       "      <td>[Why, ,, what, 's, beyond, the, border, ?]</td>\n",
       "      <td>[(Why, WRB), (,, ,), (what, WP), ('s, VBZ), (b...</td>\n",
       "      <td>[whi, ,, what, 's, beyond, the, border, ?]</td>\n",
       "      <td>[(Why, WRB), (,, ,), (what, WP), ('s, VBZ), (b...</td>\n",
       "      <td>Why_WRB ,_, what_WP 's_VBZ beyond_IN the_DT bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101059</th>\n",
       "      <td>1</td>\n",
       "      <td>[The, problem, is, like.., like, ..., Like, ,,...</td>\n",
       "      <td>[(The, DT), (problem, NN), (is, VBZ), (like..,...</td>\n",
       "      <td>[the, problem, is, like.., like, ..., like, ,,...</td>\n",
       "      <td>[[(The, DT), (problem, NN)], (is, VBZ), (like....</td>\n",
       "      <td>The_DT problem_NN is_VBZ like.._VBN like_IN .....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101060</th>\n",
       "      <td>1</td>\n",
       "      <td>[IRRC, ,, most, of, the, fuckparis, people, we...</td>\n",
       "      <td>[(IRRC, NNP), (,, ,), (most, JJS), (of, IN), (...</td>\n",
       "      <td>[irrc, ,, most, of, the, fuckpari, peopl, were...</td>\n",
       "      <td>[(IRRC, NNP), (,, ,), (most, JJS), (of, IN), (...</td>\n",
       "      <td>IRRC_NNP ,_, most_JJS of_IN the_DT fuckparis_J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101061</th>\n",
       "      <td>1</td>\n",
       "      <td>[and, no, one, wants, to, look, like, one, of,...</td>\n",
       "      <td>[(and, CC), (no, DT), (one, NN), (wants, VBZ),...</td>\n",
       "      <td>[and, no, one, want, to, look, like, one, of, ...</td>\n",
       "      <td>[(and, CC), [(no, DT), (one, NN)], (wants, VBZ...</td>\n",
       "      <td>and_CC no_DT one_NN wants_VBZ to_TO look_VB li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101062</th>\n",
       "      <td>1</td>\n",
       "      <td>[Oh, ,, just, a, little, interop, unlock, ,, O...</td>\n",
       "      <td>[(Oh, UH), (,, ,), (just, RB), (a, DT), (littl...</td>\n",
       "      <td>[oh, ,, just, a, littl, interop, unlock, ,, ok...</td>\n",
       "      <td>[(Oh, UH), (,, ,), (just, RB), [(a, DT), (litt...</td>\n",
       "      <td>Oh_UH ,_, just_RB a_DT little_JJ interop_NN un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101063</th>\n",
       "      <td>1</td>\n",
       "      <td>[Yeah, lets, just, round, them, all, up, and, ...</td>\n",
       "      <td>[(Yeah, NNP), (lets, VBZ), (just, RB), (round,...</td>\n",
       "      <td>[yeah, let, just, round, them, all, up, and, s...</td>\n",
       "      <td>[(Yeah, NNP), (lets, VBZ), (just, RB), (round,...</td>\n",
       "      <td>Yeah_NNP lets_VBZ just_RB round_VB them_PRP al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101064</th>\n",
       "      <td>1</td>\n",
       "      <td>[That, 's, the, libertarian, spirit]</td>\n",
       "      <td>[(That, DT), ('s, VBZ), (the, DT), (libertaria...</td>\n",
       "      <td>[that, 's, the, libertarian, spirit]</td>\n",
       "      <td>[(That, DT), ('s, VBZ), [(the, DT), (libertari...</td>\n",
       "      <td>That_DT 's_VBZ the_DT libertarian_JJ spirit_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101065</th>\n",
       "      <td>1</td>\n",
       "      <td>[But, did, Jesus, institute, an, invisible, Ch...</td>\n",
       "      <td>[(But, CC), (did, VBD), (Jesus, NNP), (institu...</td>\n",
       "      <td>[but, did, jesus, institut, an, invis, church,...</td>\n",
       "      <td>[(But, CC), (did, VBD), (Jesus, NNP), (institu...</td>\n",
       "      <td>But_CC did_VBD Jesus_NNP institute_VB an_DT in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101066</th>\n",
       "      <td>1</td>\n",
       "      <td>[Tell, me, more, about, how, you, do, n't, own...</td>\n",
       "      <td>[(Tell, VB), (me, PRP), (more, JJR), (about, I...</td>\n",
       "      <td>[tell, me, more, about, how, you, do, n't, own...</td>\n",
       "      <td>[(Tell, VB), (me, PRP), (more, JJR), (about, I...</td>\n",
       "      <td>Tell_VB me_PRP more_JJR about_IN how_WRB you_P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101067</th>\n",
       "      <td>1</td>\n",
       "      <td>[I, 'm, sure, they, 'll, have, a, lot, of, gre...</td>\n",
       "      <td>[(I, PRP), ('m, VBP), (sure, JJ), (they, PRP),...</td>\n",
       "      <td>[i, 'm, sure, they, ll, have, a, lot, of, grea...</td>\n",
       "      <td>[(I, PRP), ('m, VBP), (sure, JJ), (they, PRP),...</td>\n",
       "      <td>I_PRP 'm_VBP sure_JJ they_PRP 'll_MD have_VB a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101068</th>\n",
       "      <td>1</td>\n",
       "      <td>[That, ,, or, we, just, caught, the, intro, to...</td>\n",
       "      <td>[(That, DT), (,, ,), (or, CC), (we, PRP), (jus...</td>\n",
       "      <td>[that, ,, or, we, just, caught, the, intro, to...</td>\n",
       "      <td>[(That, DT), (,, ,), (or, CC), (we, PRP), (jus...</td>\n",
       "      <td>That_DT ,_, or_CC we_PRP just_RB caught_VBD th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101069</th>\n",
       "      <td>1</td>\n",
       "      <td>[Shitlord, animators, ,, thin, privilege, is, ...</td>\n",
       "      <td>[(Shitlord, NNP), (animators, NNS), (,, ,), (t...</td>\n",
       "      <td>[shitlord, anim, ,, thin, privileg, is, cartoo...</td>\n",
       "      <td>[(Shitlord, NNP), (animators, NNS), (,, ,), [(...</td>\n",
       "      <td>Shitlord_NNP animators_NNS ,_, thin_JJ privile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101070</th>\n",
       "      <td>1</td>\n",
       "      <td>[It, 's, simple, man, just, do, curls, and, fl...</td>\n",
       "      <td>[(It, PRP), ('s, VBZ), (simple, JJ), (man, NN)...</td>\n",
       "      <td>[it, 's, simpl, man, just, do, curl, and, flys...</td>\n",
       "      <td>[(It, PRP), ('s, VBZ), [(simple, JJ), (man, NN...</td>\n",
       "      <td>It_PRP 's_VBZ simple_JJ man_NN just_RB do_VB c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101071</th>\n",
       "      <td>1</td>\n",
       "      <td>[Number, 13, will, blow, your, mind, !]</td>\n",
       "      <td>[(Number, NNP), (13, CD), (will, MD), (blow, V...</td>\n",
       "      <td>[number, 13, will, blow, your, mind, !]</td>\n",
       "      <td>[(Number, NNP), (13, CD), (will, MD), (blow, V...</td>\n",
       "      <td>Number_NNP 13_CD will_MD blow_VB your_PRP$ min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101072</th>\n",
       "      <td>1</td>\n",
       "      <td>[Surprised, to, hear, them, featured, in, the,...</td>\n",
       "      <td>[(Surprised, VBN), (to, TO), (hear, VB), (them...</td>\n",
       "      <td>[surpris, to, hear, them, featur, in, the, nme]</td>\n",
       "      <td>[(Surprised, VBN), (to, TO), (hear, VB), (them...</td>\n",
       "      <td>Surprised_VBN to_TO hear_VB them_PRP featured_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101073</th>\n",
       "      <td>1</td>\n",
       "      <td>[Stay, classy, reddit, .]</td>\n",
       "      <td>[(Stay, NNP), (classy, JJ), (reddit, NN), (., .)]</td>\n",
       "      <td>[stay, classi, reddit, .]</td>\n",
       "      <td>[(Stay, NNP), [(classy, JJ), (reddit, NN)], (....</td>\n",
       "      <td>Stay_NNP classy_JJ reddit_NN ._.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101074</th>\n",
       "      <td>1</td>\n",
       "      <td>[Random, Beta, invites, for, viewers, would, b...</td>\n",
       "      <td>[(Random, NNP), (Beta, NNP), (invites, VBZ), (...</td>\n",
       "      <td>[random, beta, invit, for, viewer, would, be, ...</td>\n",
       "      <td>[(Random, NNP), (Beta, NNP), (invites, VBZ), (...</td>\n",
       "      <td>Random_NNP Beta_NNP invites_VBZ for_IN viewers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101075</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, new, robbie, schremp]</td>\n",
       "      <td>[(the, DT), (new, JJ), (robbie, NN), (schremp,...</td>\n",
       "      <td>[the, new, robbi, schremp]</td>\n",
       "      <td>[[(the, DT), (new, JJ), (robbie, NN)], [(schre...</td>\n",
       "      <td>the_DT new_JJ robbie_NN schremp_NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101076 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                            comment  \\\n",
       "0           0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1           0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2           0  [what, about, the, part, where, its, solo, que...   \n",
       "3           0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4           0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "5           0  [If, Bolvar, was, 6/7, for, 5, mana, (, silenc...   \n",
       "6           0                                            [BACON]   \n",
       "7           0              [Sounds, like, a, Kmart, customer, .]   \n",
       "8           0  [Windows, 7, ,, but, you, either, need, to, ta...   \n",
       "9           0  [Anytime, I, 've, see, this, guys, videos, I, ...   \n",
       "10          0  [I, think, thats, what, all, gas, outlets, for...   \n",
       "11          0  [You, use, the, word, ``, retarded, '', a, lot...   \n",
       "12          0  [I, still, want, him, to, start, doing, khaki,...   \n",
       "13          0  [What, about, the, bodily, autonomy, lost, whe...   \n",
       "14          0  [So, who, exactly, qualifies, as, a, minority, ?]   \n",
       "15          0               [Tire, felt, cramped, and, gtfo, 'd]   \n",
       "16          0  [I, knew, I, 'd, get, a, kick, out, of, postin...   \n",
       "17          0                                   [Off, course, !]   \n",
       "18          0                                         [Ramen, ?]   \n",
       "19          0                    [Games, would, look, retro, XD]   \n",
       "20          0  [My, mom, got, me, a, Dooney, and, Burke, Navy...   \n",
       "21          0  [If, you, work, in, IT, you, 're, gon, na, mak...   \n",
       "22          0                [The, Half-Blood, Prince, of, rock]   \n",
       "23          0  [his, work, defensively, is, really, great, ,,...   \n",
       "24          0                                          [My, BAC]   \n",
       "25          0  [A, few, T-90, 's, and, volunteers, would, n't...   \n",
       "26          0                                               [Ew]   \n",
       "27          0  [Wonder, if, he, 's, wishing, the, club, bit, ...   \n",
       "28          0  [PSN, was, down, last, night, too, ,, ended, u...   \n",
       "29          0  [``, Two, men, brutally, assaulted, and, unlaw...   \n",
       "...       ...                                                ...   \n",
       "101046      1                                 [Look, out, Teddy]   \n",
       "101047      1  [I, feel, so, much, safer, now, that, they, ha...   \n",
       "101048      1                    [OMG, WAS, IT, IN, PORTLAND, ?]   \n",
       "101049      1  [But, teachers, unions, are, the, devil, ,, yo...   \n",
       "101050      1           [Windows, 10, literally, perfect, now..]   \n",
       "101051      1          [Slightly, change, the, color, scheme, !]   \n",
       "101052      1  [Whoa, whoa, whoa, there, HITLER, MUSSOLINI, T...   \n",
       "101053      1  [And, a, woman, could, n't, get, fired, for, w...   \n",
       "101054      1  [Leave, him, alone, he, is, clearly, trans-rac...   \n",
       "101055      1  [I, 'm, sure, the, developers, ,, artists, ,, ...   \n",
       "101056      1  [Clearly, as, being, OP, is, a, VS, trait, and...   \n",
       "101057      1  [You, did, n't, include, a, picture, of, gay, ...   \n",
       "101058      1         [Why, ,, what, 's, beyond, the, border, ?]   \n",
       "101059      1  [The, problem, is, like.., like, ..., Like, ,,...   \n",
       "101060      1  [IRRC, ,, most, of, the, fuckparis, people, we...   \n",
       "101061      1  [and, no, one, wants, to, look, like, one, of,...   \n",
       "101062      1  [Oh, ,, just, a, little, interop, unlock, ,, O...   \n",
       "101063      1  [Yeah, lets, just, round, them, all, up, and, ...   \n",
       "101064      1               [That, 's, the, libertarian, spirit]   \n",
       "101065      1  [But, did, Jesus, institute, an, invisible, Ch...   \n",
       "101066      1  [Tell, me, more, about, how, you, do, n't, own...   \n",
       "101067      1  [I, 'm, sure, they, 'll, have, a, lot, of, gre...   \n",
       "101068      1  [That, ,, or, we, just, caught, the, intro, to...   \n",
       "101069      1  [Shitlord, animators, ,, thin, privilege, is, ...   \n",
       "101070      1  [It, 's, simple, man, just, do, curls, and, fl...   \n",
       "101071      1            [Number, 13, will, blow, your, mind, !]   \n",
       "101072      1  [Surprised, to, hear, them, featured, in, the,...   \n",
       "101073      1                          [Stay, classy, reddit, .]   \n",
       "101074      1  [Random, Beta, invites, for, viewers, would, b...   \n",
       "101075      1                        [the, new, robbie, schremp]   \n",
       "\n",
       "                                                  POS_tag  \\\n",
       "0       [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...   \n",
       "1       [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...   \n",
       "2       [(what, WP), (about, IN), (the, DT), (part, NN...   \n",
       "3       [(Yea, NNP), (great, JJ), (example, NN), (,, ,...   \n",
       "4       [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "5       [(If, IN), (Bolvar, NNP), (was, VBD), (6/7, CD...   \n",
       "6                                           [(BACON, NN)]   \n",
       "7       [(Sounds, NNS), (like, IN), (a, DT), (Kmart, N...   \n",
       "8       [(Windows, NNS), (7, CD), (,, ,), (but, CC), (...   \n",
       "9       [(Anytime, NNP), (I, PRP), ('ve, VBP), (see, V...   \n",
       "10      [(I, PRP), (think, VBP), (thats, NNS), (what, ...   \n",
       "11      [(You, PRP), (use, VBP), (the, DT), (word, NN)...   \n",
       "12      [(I, PRP), (still, RB), (want, VBP), (him, PRP...   \n",
       "13      [(What, WP), (about, IN), (the, DT), (bodily, ...   \n",
       "14      [(So, RB), (who, WP), (exactly, RB), (qualifie...   \n",
       "15      [(Tire, NNP), (felt, VBD), (cramped, VBN), (an...   \n",
       "16      [(I, PRP), (knew, VBD), (I, PRP), ('d, MD), (g...   \n",
       "17                      [(Off, IN), (course, NN), (!, .)]   \n",
       "18                                 [(Ramen, NNS), (?, .)]   \n",
       "19      [(Games, NNS), (would, MD), (look, VB), (retro...   \n",
       "20      [(My, PRP$), (mom, NN), (got, VBD), (me, PRP),...   \n",
       "21      [(If, IN), (you, PRP), (work, VBP), (in, IN), ...   \n",
       "22      [(The, DT), (Half-Blood, NNP), (Prince, NNP), ...   \n",
       "23      [(his, PRP$), (work, NN), (defensively, RB), (...   \n",
       "24                                [(My, PRP$), (BAC, NN)]   \n",
       "25      [(A, DT), (few, JJ), (T-90, NNP), ('s, POS), (...   \n",
       "26                                             [(Ew, NN)]   \n",
       "27      [(Wonder, VB), (if, IN), (he, PRP), ('s, VBZ),...   \n",
       "28      [(PSN, NN), (was, VBD), (down, RB), (last, JJ)...   \n",
       "29      [(``, ``), (Two, CD), (men, NNS), (brutally, R...   \n",
       "...                                                   ...   \n",
       "101046              [(Look, VB), (out, RP), (Teddy, NNP)]   \n",
       "101047  [(I, PRP), (feel, VBP), (so, RB), (much, JJ), ...   \n",
       "101048  [(OMG, NNP), (WAS, NNP), (IT, NNP), (IN, NNP),...   \n",
       "101049  [(But, CC), (teachers, NNS), (unions, NNS), (a...   \n",
       "101050  [(Windows, NNS), (10, CD), (literally, RB), (p...   \n",
       "101051  [(Slightly, RB), (change, VBP), (the, DT), (co...   \n",
       "101052  [(Whoa, NNP), (whoa, NN), (whoa, NN), (there, ...   \n",
       "101053  [(And, CC), (a, DT), (woman, NN), (could, MD),...   \n",
       "101054  [(Leave, NNP), (him, PRP), (alone, RB), (he, P...   \n",
       "101055  [(I, PRP), ('m, VBP), (sure, JJ), (the, DT), (...   \n",
       "101056  [(Clearly, RB), (as, IN), (being, VBG), (OP, N...   \n",
       "101057  [(You, PRP), (did, VBD), (n't, RB), (include, ...   \n",
       "101058  [(Why, WRB), (,, ,), (what, WP), ('s, VBZ), (b...   \n",
       "101059  [(The, DT), (problem, NN), (is, VBZ), (like..,...   \n",
       "101060  [(IRRC, NNP), (,, ,), (most, JJS), (of, IN), (...   \n",
       "101061  [(and, CC), (no, DT), (one, NN), (wants, VBZ),...   \n",
       "101062  [(Oh, UH), (,, ,), (just, RB), (a, DT), (littl...   \n",
       "101063  [(Yeah, NNP), (lets, VBZ), (just, RB), (round,...   \n",
       "101064  [(That, DT), ('s, VBZ), (the, DT), (libertaria...   \n",
       "101065  [(But, CC), (did, VBD), (Jesus, NNP), (institu...   \n",
       "101066  [(Tell, VB), (me, PRP), (more, JJR), (about, I...   \n",
       "101067  [(I, PRP), ('m, VBP), (sure, JJ), (they, PRP),...   \n",
       "101068  [(That, DT), (,, ,), (or, CC), (we, PRP), (jus...   \n",
       "101069  [(Shitlord, NNP), (animators, NNS), (,, ,), (t...   \n",
       "101070  [(It, PRP), ('s, VBZ), (simple, JJ), (man, NN)...   \n",
       "101071  [(Number, NNP), (13, CD), (will, MD), (blow, V...   \n",
       "101072  [(Surprised, VBN), (to, TO), (hear, VB), (them...   \n",
       "101073  [(Stay, NNP), (classy, JJ), (reddit, NN), (., .)]   \n",
       "101074  [(Random, NNP), (Beta, NNP), (invites, VBZ), (...   \n",
       "101075  [(the, DT), (new, JJ), (robbie, NN), (schremp,...   \n",
       "\n",
       "                                                     stem  \\\n",
       "0       [hitsugaya, be, in, everi, fight, is, part, be...   \n",
       "1       [i, imagin, his, nicknam, in, prison, would, b...   \n",
       "2       [what, about, the, part, where, it, solo, queu...   \n",
       "3       [yea, great, exampl, ,, i, see, gold, player, ...   \n",
       "4       [well, it, must, ve, been, a, glitch, then, ,,...   \n",
       "5       [if, bolvar, was, 6/7, for, 5, mana, (, silenc...   \n",
       "6                                                 [bacon]   \n",
       "7                      [sound, like, a, kmart, custom, .]   \n",
       "8       [window, 7, ,, but, you, either, need, to, tak...   \n",
       "9       [anytim, i, ve, see, this, guy, video, i, want...   \n",
       "10      [i, think, that, what, all, gas, outlet, for, ...   \n",
       "11       [you, use, the, word, ``, retard, '', a, lot, .]   \n",
       "12      [i, still, want, him, to, start, do, khaki, co...   \n",
       "13      [what, about, the, bodili, autonomi, lost, whe...   \n",
       "14             [so, who, exact, qualifi, as, a, minor, ?]   \n",
       "15                     [tire, felt, cramp, and, gtfo, 'd]   \n",
       "16      [i, knew, i, 'd, get, a, kick, out, of, post, ...   \n",
       "17                                        [off, cours, !]   \n",
       "18                                             [ramen, ?]   \n",
       "19                         [game, would, look, retro, xd]   \n",
       "20      [my, mom, got, me, a, dooney, and, burk, navi,...   \n",
       "21      [if, you, work, in, it, you, re, gon, na, make...   \n",
       "22                     [the, half-blood, princ, of, rock]   \n",
       "23      [his, work, defens, is, realli, great, ,, he, ...   \n",
       "24                                              [my, bac]   \n",
       "25      [a, few, t-90, 's, and, volunt, would, n't, hu...   \n",
       "26                                                   [ew]   \n",
       "27      [wonder, if, he, 's, wish, the, club, bit, on,...   \n",
       "28      [psn, was, down, last, night, too, ,, end, up,...   \n",
       "29      [``, two, men, brutal, assault, and, unlaw, de...   \n",
       "...                                                   ...   \n",
       "101046                                 [look, out, teddi]   \n",
       "101047  [i, feel, so, much, safer, now, that, they, ha...   \n",
       "101048                    [omg, was, it, in, portland, ?]   \n",
       "101049  [but, teacher, union, are, the, devil, ,, you,...   \n",
       "101050                [window, 10, liter, perfect, now..]   \n",
       "101051             [slight, chang, the, color, scheme, !]   \n",
       "101052  [whoa, whoa, whoa, there, hitler, mussolini, t...   \n",
       "101053  [and, a, woman, could, n't, get, fire, for, wh...   \n",
       "101054  [leav, him, alon, he, is, clear, trans-rac, ,,...   \n",
       "101055  [i, 'm, sure, the, develop, ,, artist, ,, and,...   \n",
       "101056  [clear, as, be, op, is, a, vs, trait, and, und...   \n",
       "101057  [you, did, n't, includ, a, pictur, of, gay, pe...   \n",
       "101058         [whi, ,, what, 's, beyond, the, border, ?]   \n",
       "101059  [the, problem, is, like.., like, ..., like, ,,...   \n",
       "101060  [irrc, ,, most, of, the, fuckpari, peopl, were...   \n",
       "101061  [and, no, one, want, to, look, like, one, of, ...   \n",
       "101062  [oh, ,, just, a, littl, interop, unlock, ,, ok...   \n",
       "101063  [yeah, let, just, round, them, all, up, and, s...   \n",
       "101064               [that, 's, the, libertarian, spirit]   \n",
       "101065  [but, did, jesus, institut, an, invis, church,...   \n",
       "101066  [tell, me, more, about, how, you, do, n't, own...   \n",
       "101067  [i, 'm, sure, they, ll, have, a, lot, of, grea...   \n",
       "101068  [that, ,, or, we, just, caught, the, intro, to...   \n",
       "101069  [shitlord, anim, ,, thin, privileg, is, cartoo...   \n",
       "101070  [it, 's, simpl, man, just, do, curl, and, flys...   \n",
       "101071            [number, 13, will, blow, your, mind, !]   \n",
       "101072    [surpris, to, hear, them, featur, in, the, nme]   \n",
       "101073                          [stay, classi, reddit, .]   \n",
       "101074  [random, beta, invit, for, viewer, would, be, ...   \n",
       "101075                         [the, new, robbi, schremp]   \n",
       "\n",
       "                                        noun_phrase_chunk  \\\n",
       "0       [(Hitsugaya, NNP), (being, VBG), (in, IN), [(e...   \n",
       "1       [(I, PRP), (imagine, VBP), (his, PRP$), [(nick...   \n",
       "2       [(what, WP), (about, IN), [(the, DT), (part, N...   \n",
       "3       [(Yea, NNP), [(great, JJ), (example, NN)], (,,...   \n",
       "4       [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "5       [(If, IN), (Bolvar, NNP), (was, VBD), (6/7, CD...   \n",
       "6                                         [[(BACON, NN)]]   \n",
       "7       [(Sounds, NNS), (like, IN), (a, DT), (Kmart, N...   \n",
       "8       [(Windows, NNS), (7, CD), (,, ,), (but, CC), (...   \n",
       "9       [(Anytime, NNP), (I, PRP), ('ve, VBP), (see, V...   \n",
       "10      [(I, PRP), (think, VBP), (thats, NNS), (what, ...   \n",
       "11      [(You, PRP), (use, VBP), [(the, DT), (word, NN...   \n",
       "12      [(I, PRP), (still, RB), (want, VBP), (him, PRP...   \n",
       "13      [(What, WP), (about, IN), [(the, DT), (bodily,...   \n",
       "14      [(So, RB), (who, WP), (exactly, RB), (qualifie...   \n",
       "15      [(Tire, NNP), (felt, VBD), (cramped, VBN), (an...   \n",
       "16      [(I, PRP), (knew, VBD), (I, PRP), ('d, MD), (g...   \n",
       "17                    [(Off, IN), [(course, NN)], (!, .)]   \n",
       "18                                 [(Ramen, NNS), (?, .)]   \n",
       "19      [(Games, NNS), (would, MD), (look, VB), [(retr...   \n",
       "20      [(My, PRP$), [(mom, NN)], (got, VBD), (me, PRP...   \n",
       "21      [(If, IN), (you, PRP), (work, VBP), (in, IN), ...   \n",
       "22      [(The, DT), (Half-Blood, NNP), (Prince, NNP), ...   \n",
       "23      [(his, PRP$), [(work, NN)], (defensively, RB),...   \n",
       "24                              [(My, PRP$), [(BAC, NN)]]   \n",
       "25      [(A, DT), (few, JJ), (T-90, NNP), ('s, POS), (...   \n",
       "26                                           [[(Ew, NN)]]   \n",
       "27      [(Wonder, VB), (if, IN), (he, PRP), ('s, VBZ),...   \n",
       "28      [[(PSN, NN)], (was, VBD), (down, RB), [(last, ...   \n",
       "29      [(``, ``), (Two, CD), (men, NNS), (brutally, R...   \n",
       "...                                                   ...   \n",
       "101046              [(Look, VB), (out, RP), (Teddy, NNP)]   \n",
       "101047  [(I, PRP), (feel, VBP), (so, RB), [(much, JJ),...   \n",
       "101048  [(OMG, NNP), (WAS, NNP), (IT, NNP), (IN, NNP),...   \n",
       "101049  [(But, CC), (teachers, NNS), (unions, NNS), (a...   \n",
       "101050  [(Windows, NNS), (10, CD), (literally, RB), [(...   \n",
       "101051  [(Slightly, RB), (change, VBP), [(the, DT), (c...   \n",
       "101052  [(Whoa, NNP), [(whoa, NN)], [(whoa, NN)], (the...   \n",
       "101053  [(And, CC), [(a, DT), (woman, NN)], (could, MD...   \n",
       "101054  [(Leave, NNP), (him, PRP), (alone, RB), (he, P...   \n",
       "101055  [(I, PRP), ('m, VBP), (sure, JJ), (the, DT), (...   \n",
       "101056  [(Clearly, RB), (as, IN), (being, VBG), (OP, N...   \n",
       "101057  [(You, PRP), (did, VBD), (n't, RB), (include, ...   \n",
       "101058  [(Why, WRB), (,, ,), (what, WP), ('s, VBZ), (b...   \n",
       "101059  [[(The, DT), (problem, NN)], (is, VBZ), (like....   \n",
       "101060  [(IRRC, NNP), (,, ,), (most, JJS), (of, IN), (...   \n",
       "101061  [(and, CC), [(no, DT), (one, NN)], (wants, VBZ...   \n",
       "101062  [(Oh, UH), (,, ,), (just, RB), [(a, DT), (litt...   \n",
       "101063  [(Yeah, NNP), (lets, VBZ), (just, RB), (round,...   \n",
       "101064  [(That, DT), ('s, VBZ), [(the, DT), (libertari...   \n",
       "101065  [(But, CC), (did, VBD), (Jesus, NNP), (institu...   \n",
       "101066  [(Tell, VB), (me, PRP), (more, JJR), (about, I...   \n",
       "101067  [(I, PRP), ('m, VBP), (sure, JJ), (they, PRP),...   \n",
       "101068  [(That, DT), (,, ,), (or, CC), (we, PRP), (jus...   \n",
       "101069  [(Shitlord, NNP), (animators, NNS), (,, ,), [(...   \n",
       "101070  [(It, PRP), ('s, VBZ), [(simple, JJ), (man, NN...   \n",
       "101071  [(Number, NNP), (13, CD), (will, MD), (blow, V...   \n",
       "101072  [(Surprised, VBN), (to, TO), (hear, VB), (them...   \n",
       "101073  [(Stay, NNP), [(classy, JJ), (reddit, NN)], (....   \n",
       "101074  [(Random, NNP), (Beta, NNP), (invites, VBZ), (...   \n",
       "101075  [[(the, DT), (new, JJ), (robbie, NN)], [(schre...   \n",
       "\n",
       "                                           joined_POS_tag  \n",
       "0       Hitsugaya_NNP being_VBG in_IN every_DT fight_N...  \n",
       "1       I_PRP imagine_VBP his_PRP$ nickname_NN in_IN p...  \n",
       "2       what_WP about_IN the_DT part_NN where_WRB its_...  \n",
       "3       Yea_NNP great_JJ example_NN ,_, I_PRP see_VBP ...  \n",
       "4       Well_IN it_PRP must_MD 've_VBP been_VBN a_DT g...  \n",
       "5       If_IN Bolvar_NNP was_VBD 6/7_CD for_IN 5_CD ma...  \n",
       "6                                                BACON_NN  \n",
       "7       Sounds_NNS like_IN a_DT Kmart_NNP customer_NN ._.  \n",
       "8       Windows_NNS 7_CD ,_, but_CC you_PRP either_RB ...  \n",
       "9       Anytime_NNP I_PRP 've_VBP see_VB this_DT guys_...  \n",
       "10      I_PRP think_VBP thats_NNS what_WP all_DT gas_N...  \n",
       "11      You_PRP use_VBP the_DT word_NN ``_`` retarded_...  \n",
       "12      I_PRP still_RB want_VBP him_PRP to_TO start_VB...  \n",
       "13      What_WP about_IN the_DT bodily_JJ autonomy_NN ...  \n",
       "14      So_RB who_WP exactly_RB qualifies_VBZ as_IN a_...  \n",
       "15      Tire_NNP felt_VBD cramped_VBN and_CC gtfo_JJ '...  \n",
       "16      I_PRP knew_VBD I_PRP 'd_MD get_VB a_DT kick_NN...  \n",
       "17                                   Off_IN course_NN !_.  \n",
       "18                                          Ramen_NNS ?_.  \n",
       "19              Games_NNS would_MD look_VB retro_JJ XD_NN  \n",
       "20      My_PRP$ mom_NN got_VBD me_PRP a_DT Dooney_NNP ...  \n",
       "21      If_IN you_PRP work_VBP in_IN IT_NNP you_PRP 'r...  \n",
       "22         The_DT Half-Blood_NNP Prince_NNP of_IN rock_NN  \n",
       "23      his_PRP$ work_NN defensively_RB is_VBZ really_...  \n",
       "24                                         My_PRP$ BAC_NN  \n",
       "25      A_DT few_JJ T-90_NNP 's_POS and_CC volunteers_...  \n",
       "26                                                  Ew_NN  \n",
       "27      Wonder_VB if_IN he_PRP 's_VBZ wishing_VBG the_...  \n",
       "28      PSN_NN was_VBD down_RB last_JJ night_NN too_RB...  \n",
       "29      ``_`` Two_CD men_NNS brutally_RB assaulted_VBD...  \n",
       "...                                                   ...  \n",
       "101046                           Look_VB out_RP Teddy_NNP  \n",
       "101047  I_PRP feel_VBP so_RB much_JJ safer_NN now_RB t...  \n",
       "101048     OMG_NNP WAS_NNP IT_NNP IN_NNP PORTLAND_NNP ?_.  \n",
       "101049  But_CC teachers_NNS unions_NNS are_VBP the_DT ...  \n",
       "101050  Windows_NNS 10_CD literally_RB perfect_JJ now....  \n",
       "101051  Slightly_RB change_VBP the_DT color_NN scheme_...  \n",
       "101052  Whoa_NNP whoa_NN whoa_NN there_RB HITLER_NNP M...  \n",
       "101053  And_CC a_DT woman_NN could_MD n't_RB get_VB fi...  \n",
       "101054  Leave_NNP him_PRP alone_RB he_PRP is_VBZ clear...  \n",
       "101055  I_PRP 'm_VBP sure_JJ the_DT developers_NNS ,_,...  \n",
       "101056  Clearly_RB as_IN being_VBG OP_NNP is_VBZ a_DT ...  \n",
       "101057  You_PRP did_VBD n't_RB include_VB a_DT picture...  \n",
       "101058  Why_WRB ,_, what_WP 's_VBZ beyond_IN the_DT bo...  \n",
       "101059  The_DT problem_NN is_VBZ like.._VBN like_IN .....  \n",
       "101060  IRRC_NNP ,_, most_JJS of_IN the_DT fuckparis_J...  \n",
       "101061  and_CC no_DT one_NN wants_VBZ to_TO look_VB li...  \n",
       "101062  Oh_UH ,_, just_RB a_DT little_JJ interop_NN un...  \n",
       "101063  Yeah_NNP lets_VBZ just_RB round_VB them_PRP al...  \n",
       "101064     That_DT 's_VBZ the_DT libertarian_JJ spirit_NN  \n",
       "101065  But_CC did_VBD Jesus_NNP institute_VB an_DT in...  \n",
       "101066  Tell_VB me_PRP more_JJR about_IN how_WRB you_P...  \n",
       "101067  I_PRP 'm_VBP sure_JJ they_PRP 'll_MD have_VB a...  \n",
       "101068  That_DT ,_, or_CC we_PRP just_RB caught_VBD th...  \n",
       "101069  Shitlord_NNP animators_NNS ,_, thin_JJ privile...  \n",
       "101070  It_PRP 's_VBZ simple_JJ man_NN just_RB do_VB c...  \n",
       "101071  Number_NNP 13_CD will_MD blow_VB your_PRP$ min...  \n",
       "101072  Surprised_VBN to_TO hear_VB them_PRP featured_...  \n",
       "101073                   Stay_NNP classy_JJ reddit_NN ._.  \n",
       "101074  Random_NNP Beta_NNP invites_VBZ for_IN viewers...  \n",
       "101075                 the_DT new_JJ robbie_NN schremp_NN  \n",
       "\n",
       "[101076 rows x 6 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_tag = []\n",
    "for index, row in chosen_data.iterrows():\n",
    "    joined_tag.append(' '.join([word + \"_\" + pos for word, pos in row['POS_tag']]))\n",
    "chosen_data['joined_POS_tag'] = joined_tag.copy()\n",
    "chosen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
